Title:
Scope: What is the general context?
Problem: What is the specific problem?
Significance: Why is it a problem? What are the reasons why it is a problem?
Solution: How do you solve it?
Evaluation: Does your solution fulfill expectations (very short)?

Scope: 
Agent training happens in a closed off environment on a specified market and against a set number of opponents.
Before releasing agents onto a real market, their performance and robustness must be measured.

Problem: RL Agents have been trained extensively and over long periods of time, on a certain marketplace and against
a set number of competitors. But before releasing them, you need to be sure that they fulfill your expectations and 
can produce reliably good results, also when compared against other agents, both Rule-based and RL, which were
not part of their training.
How do you build a pipeline that can monitor and evaluate an agents performance after it has been trained,
in order to classify and compare it to other agents? What metrics do you employ to make these classifications?
How do you evaluate monitoring results without a universal truth (There is no "correct answer")?

Significance: Training and playing around with agents in a simulated environment is risk-free - they do not
interact with the real world and can therefore not influence real market states.
But the ultimate goal of training these agents is to be able to give them real responsibilies and power over 
a companies pricing policy. Before this transition into the real world market can happen, it needs to be made sure that
agents are robust and can deal with any type of situation that may arise. Part of this process is making sure that 
the simulated marketplace is as realistic as possible, but the major part that can and must win over customers (as in
companies wanting to employ the agent to set their prices) is a display of continued profitability in different circumstances.
This can not be achieved by training alone, but trained agents must then be put through rigorous stages of monitoring and
evaluation to prove their capabilities.

Solution:
In our framework, we employed monitoring on both a Macro (Agent Monitoring) as well as a Micro (Exampleprinter) level 
as well as a static tool (Policyanalyzer).
The Macro level is achieved by running a large number of episodes and recording and classifying all actions and results of
the actions by all participating agents. These metrics are then evaluated and allow for a large-scale analysis of an
agents performance, most notably by being able to compare the overall profits against those fo other agents.
The Micro level concerns itself with monitoring each and every price set by the agent within a relatively small time window
of one episode == x time steps == x times setting a price. This type of comparison can be for example used to see whether or not an 
agent continuously undercuts prices of their competitors or forms a cartel with them. You could say that the Micro level concerns
itself with the "personality" of the agent.
The Policyanalyzer can be used to get an overview of how an agent would react to different market states, for example the 
price he would set for a new product if their only competitor sets a number of other prices, or depending on how many sales
were made in the last time step.

Evaluation: Using our monitoring framework, it is 

"Github Issues":
- Start monitoring automatically after training
- CSV results of monitoring
	- Ability to feed in CSV and get graphs/evaluation
- Compare different training stages to know when progress stagnates
- Not only map profits in agent_monitoring, but also prices set, inventory etc.
- Monitor agents playing on the same marketplace, not parallel to each other (agent_monitoring)
- Text-based evaluations of monitoring results
- Automatic classifications/grading
- Grid search in conjunction with monitoring for automatic adjustments of parameters