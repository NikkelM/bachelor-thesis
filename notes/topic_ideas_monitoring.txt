Title:
Scope: What is the general context?
Problem: What is the specific problem?
Significance: Why is it a problem? What are the reasons why it is a problem?
Solution: How do you solve it?
Evaluation: Does your solution fulfill expectations (very short)?

Title Ideas for Monitoring:
"Visualizing the performance of Reinforcement-Learning Agents"
"Estimating the quality of Reinforcement-Learning Agents"


Scope: 
Agent training happens in a closed off environment on a specified market and against a set number of opponents.
Before releasing agents onto a real market, their performance and robustness must be measured.
RL Agents are trained extensively and over long periods of time, on a certain marketplace and against
a set number of competitors. The only metric you have during training is the agents mean profit after each episode,
but before releasing them, you need to be sure that they fulfill your expectations and 
can produce reliably good results, also when compared against other agents, both Rule-based and RL, which were
not part of their training. For this, you need to monitor a number of different metrics extensively.

Problem: 
How do you build a pipeline that can monitor and evaluate an agents performance after it has been trained,
in order to classify and compare it to other agents? 
What metrics do you employ to make these classifications?
How do you evaluate and *visualize* learnings from the monitoring process? 
How do you evaluate monitoring results without a universal truth (There is no "correct answer")?
	- How do you differentiate good agents from bad agents using these results?
How do you incorporate learnings from the monitoring results to produce better agents in the future?

Significance: 
Training and playing around with agents in a simulated environment is risk-free - they do not
interact with the real world and can therefore not influence real market states.
But the ultimate goal of training these agents is to be able to give them real responsibilities and power over 
a companies pricing policy. Before this transition into the real world market can happen, it needs to be made sure that
agents are robust and can deal with any type of situation that may arise. Part of this process is making sure that 
the simulated marketplace is as realistic as possible, but the major part that can and must win over customers (as in
companies wanting to employ the agent to set their prices) is a display of continued profitability in different circumstances.
Trained agents must therefore be put through rigorous stages of monitoring and evaluation to prove their capabilities after
training has completed, to make sure that performance during training was not due to outliers or anomalies, but due to a
superior pricing policy by the agent.

Solution:
In our framework, we employed monitoring on both a Macro (Agent Monitoring) as well as a Micro (Exampleprinter) level 
as well as using static tool(s) (Policyanalyzer).
The Macro level is achieved by running a large number of episodes and recording and classifying all actions and results of
the actions by all participating agents. These metrics are then evaluated and allow for a large-scale analysis of an
agents performance, most notably by being able to compare the overall profits against those of other agents.
The Micro level concerns itself with monitoring each and every price set by the agent within a relatively small time window
of one episode == x time steps == x times setting a price. This type of comparison can be for example used to see whether or not an 
agent continuously undercuts prices of their competitors or forms a cartel with them. You could say that the Micro level concerns
itself with the "personality" of the agent.
The Policyanalyzer can be used to get an overview of how an agent would react to different market states, for example the 
price he would set for a new product if their only competitor sets a number of other prices, or depending on how many sales
were made in the last time step.

Evaluation:
Our monitoring framework offers an all-in-one solution for training and evaluating agents. During training, we save
the best models in their respective timeframe (e.g. one model saved each 10000 episodes).
After training, we can evaluate and compare these models not only with each other, but also with other agents, both RL and rule-based,
which comes close to a real market scenario, which has a mix of human actors, rule-based approaches and possibly other, rival RL-agents.
Our tools can help to find areas in which the agents still struggle and thereby give pointers to what may need to
change in the next iteration. [this would require knowledge about which hyperparameters were changed leading to what change in model quality].


What makes a good model?
	- Consistently high rewards
	- Few to no outliers - each one is a dent in the companies profit
		- See which state and actions led to the outlier using Micro-analysis coming from Macro

"Github Issues" -> things that we could work on aiding this thesis concept:
- Start monitoring automatically after training
- CSV results of monitoring
	- Ability to feed in CSV and get graphs/evaluation
- Compare different training stages to know when progress stagnates
- Not only map profits in agent_monitoring, but also prices set, inventory etc.
- Monitor agents playing on the same marketplace, not parallel to each other (agent_monitoring)
- Text-based evaluations of monitoring results
- Automatic classifications/grading
- Grid search in conjunction with monitoring for automatic adjustments of parameters
