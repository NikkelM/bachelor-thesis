\label{chapter:Approaches}
\begin{jointwork}
	In this section we will take a look at the different approaches we took to monitoring agents in our framework, explaining the reasons why we chose to implement specific features and how they help us in determining an agents strengths and weaknesses.
\end{jointwork}

\section{When to monitor what} \label{section:WhenToMonitorWhat}

Our workflow (which will be explained in more detail in \nameref{chapter:OurWorkflow}) can generally be split into two parts when it comes to monitoring and evaluation of agents. In the future, when talking about the \emph{workflow} we will be talking about the process of configuring and starting a training session, where a Reinforcement-Learning agent is being trained on a specific marketplace against competitors. The \emph{workflow} also includes the subsequent collection of data used to evaluate the agent's performance. We are also introducing the term of the \emph{complete agent} in this section, which will be used to refer to both Reinforcement-Learning agents that have been fully trained as well as rule-based agents which do not need training.

\begin{enumerate}
	\item During training: Having data available as soon as possible without having to wait for a long training session to end is crucial to an efficient workflow. Our framework enables us to analyze and visualize data while a training session is still running \todo{Implement this feature. It should at least be able to temporarily halt the training to start an intermediate monitoring session}. This enables us to filter out agents with sub-par performance prematurely. This can be done using user-defined threshholds or on the basis of previoulsly collected data (e.g. only keeping agents that are performing better than at least 50\% of other agents after the same amount of training in comparable scenarios.) \todo{Also implement this feature. Allow users to set rules for when a training session should be terminated if the agent is not performing well at a certain point. Also, it should be able to give the program a set of datapoints and have it set rules if possible}

	\item On complete agents: After a training session has finished we have a complete and final set of data available for an agent, which enables us to perform more thorough and reliable tests. These can include simulating runs of a marketplace to gather data on the agent's performance in different scenarios and against different competitors, or running a static analysis of the agent's policy in different market states. The tools available for trained agents are in the same way also usable on rule-based agents.
\end{enumerate}

In the following sections, we will take a look at all of the tools our framework provides for monitoring agents, distinguishing between the two general types of monitoring mentioned above. The goal of this section is to give a short overview of each tool, how and why they were implemented and what value they offer to the framework as a whole and to the analysis of agent reliability and robustness in particular. We will also discuss features that are currently not available, explaining how they could benefit the entire workflow or enrich the overall experience.

\section{Monitoring during a training session}

When talking about monitoring agents during a training session, we are of course talking about Reinforcement-Learning agents, since Rule-Based agents always perform the same and cannot be trained. \todo{Is this a footnote? This info should be obvious, but maybe should be mentioned for completeness?} Monitoring agents while they are still being trained enables us to be more closely connected to the training process. Ultimately, the goal of such monitoring tools is to be able to predict the estimated "quality" of the final trained agent as reliably as possible while the training is still going. Users must however be careful when interpreting the results of monitoring tools that work on "incomplete" agents, we will go more into this  in \nameref{chapter:InterpretResults}.

\subsection{Tensorboard}

The \emph{Tensorboard} is an external tool from the from the Reinforcement-Learning library \emph{Tensorflow}\footnote[0][-0.2]{https://www.tensorflow.org/}. With just a few lines of code \todo{Question for the tutors: Do I give an example of such code?} a training session can be connected to a Tensorboard. We are then able to pass any number of parameters and metrics we deem interesting or useful to the Tensorboard, which then offers visualizations for each of them, updating live as the training progresses. Though the Tensorboard does not interpret data in any way, it is an immensely useful tool for quickly and easily recording data and offering a first rough comparison of competitors in the market.
\todo{Create some sample diagrams. Perhaps these can be re-used in the workflow section, so they could perhaps be moved to the Appendix for reference?}

\subsection{Live-monitoring}

Unlike the Tensorboard, the monitoring tools summarised under the term \emph{live-monitoring} were completely and from the ground up built by our team. For most of the visualizations, the \emph{matplotlib}\footnote[0][-0.2]{https://matplotlib.org/stable/index.html} library was used. Live-monitoring aims to be more configurable and in-depth with the metrics it offers than the aforementioned Tensorboard. \todo{make the current live-monitoring so that graphs are actually being created while the training is running, not like it is now with graphs only being created afterwards} During a training session, "intermediate" models, as we will call them, are being saved after a set number of episodes. \todo{Have we introduced the concept of episodes before? Should a Glossary be introduced, or do we do this stuff in the introductions?} These models contain the current policy of the agent and can be used the same as models of complete agents, the only difference being the quality of the agent, as those with a lower amount of trained episodes generally perform worse. \todo{Does this need a citation?} These intermediate models can then be used by a range of monitoring tools available to us. Since the models only contain the current policy of an agent but not the history of states and actions preceding the model, we need to run separate simulations on these models to be able to analyze and evaluate them. For this, we utilize our \emph{agent-monitoring} toolset, which will be explained in more detail in the \nameref{subsection:AgentMonitoring} section.

\section{Monitoring complete agents}

The following tools offer a wide range of functionalities for monitoring complete agents. We can use these tools to both determine the reliability and robustness of one certain agent, but also to compare different agents, either during a joined monitoring session or by comparing results of monitoring runs on identically parametrized market situations.

\subsection{Agent-monitoring} \label{subsection:AgentMonitoring}

Using the \emph{Agent-monitoring} toolset, users can configure a custom market simulation, using the following parameters:

\begin{enumerate}
	\item Episodes: This parameter decides how many independent simulations are run in sequence. At the start of each episode, the market state will be reset. Within an episode, each vendor runs through 50 timesteps, during each of which a price is set (depending on the chosen economy type, this can include a rebuy price for used items) and a set number of customers interact with the vendors.
	\item Plot interval \todo{Currently, the plot interval is not used by anything, since the statistics-plots have been "removed" in favor of the tensorboard plots}: A number of diagram types enable the user to view averaged or aggregated data over a period of time. The plot interval parameter decides the size of these intervals. Smaller intervals mean more accurate but also more convoluted data points. Computational time also increases with a smaller interval size.
	\item Marketplace: Using this parameter, the user can set the marketplace on which the monitoring session will be run. There are marketplaces available for each combination of the following features:
	      \begin{enumerate}
		      \item Marketplace type: Linear Economy, Circular Economy, Circular Economy with rebuy-prices
		      \item Market environment: Monopoly, Duopoly, Oligopoly
	      \end{enumerate}
	\item Agents: Depending on the chosen marketplace, only a select number of valid agents can be chosen to be monitored, as each agent is built to interact with a specific type of marketplace. First off, all agents belong to one of the two major categories: \emph{Reinforcement Learning agent} or \emph{Rulebased agent}.
\end{enumerate}

It is important to note that while multiple agents are being monitored simultaneously, each agent operates on a market of its own. This means that while each agent receives the same initial market state at the start of an episode, and all agents play against the same competitors, monitored agents do not play against each other. The goal of the \emph{Agent-monitoring} tool is to be able to compare different agents playing under the same circumstances to determine which ones might be "better" than others. This would not be achievable with monitored agents playing against each other, as the assumptions about the market state would differ for each agent.

During each episode and for each vendor, all actions and market events are being recorded using \emph{watchers}.\todo{Do I keep this new word? If yes, it should be explained at least a little bit!} At the end of a monitoring session, the collected data is evaluated in different visual formats. First of all, all data that would be available to see in the \emph{tensorboard} during a training session is visualized using density plots. These plots can be used to compare the vendors in a more granular way, if for example the effect of a parameter on the customer's sell-back behaviour of used items should be tested. Another view that is created is a bar diagram containing the cumulative profits per episode for each agent plotted against each other. This allows for a quick overview to see which agent had an overall better performance.

The most common usecase of the \emph{Agent-monitoring} tool is to test trained agents against competitors other than the ones it was trained against. This is done to test the \emph{Reliability} of the agent, an important factor in deciding an agent's quality, as its competitors in the real market will differ from any it has encountered in training, due to the sheer vastness of options when it comes to dynamic pricing models available nowadays, see \nameref{section:ExplainVendors}. 
\subsection{Exampleprinter}

The \emph{Exampleprinter} is a tool meant for quickly evaluating an agent in-depth. When run, each action the monitored agent takes is being recorded, in addition to market states and events, such as the number of customers arriving and the amount of products thrown away. For certain market scenarios such as the \emph{CircularEconomyRebuyPriceDuopoly}, which simulates a circular economy model with rebuy prices in which two vendors compete against each other, these actions and events are also being summarised as an animated graphic, where each time-step is being illustrated. \todo{Add in a graphic here. Perhaps have two of the time-steps to "simulate" the animation}

\subsection{Policyanalyzer}

The \emph{Policyanalyzer} is our only tool which does not simulate a run of the market scenario. Instead, the tool can be used to monitor an agent's reaction to different market events. The user can decide on up to two different features to give as an input, such as the competitor's new and used prices, and the Policyanalyzer will feed all possible input combinations to the agent and record its reactions. \todo{Diagram}

\section{Features for the future}

This section is meant as a collection of ideas and approaches for tools that could further enhance the workflow, but which have not been implemented and therefore not been tested for their feasability and usefulness.