\begin{jointwork}\label{ch:Approaches}
	In this chapter we will take a look at the different approaches we took to monitoring agents in our framework, explaining the reasons why we chose to implement specific features and how they help us in determining an agent's strengths and weaknesses.
\end{jointwork}

\section{When to monitor what}\label{sec:WhenToMonitorWhat}

Our \emph{workflow} (which will be explained in more detail in \cbfref{ch:OurWorkflow}) can generally be split into two parts when it comes to monitoring and evaluating agents: \emph{during} and \emph{after} training. When talking about the \emph{workflow} we refer to the process of configuring and starting a training session, where a Reinforcement learning agent is being trained on a specific marketplace against competitors. The \emph{workflow} also includes the subsequent collection of data used to evaluate an agent's performance. We are also introducing the term of the \emph{complete agent} in this section, which will be used to refer to both RL agents that have completed a training run and rule-based agents, which do not need training.

As mentioned above, we split our monitoring tools into the following two categories:

\begin{enumerate}
	\item \textbf{During training} (\bfref{sec:DuringTraining}): Having data available as soon as possible without having to wait for a long training session to end is crucial to an efficient workflow. Our framework enables us to collect and visualise data while a training session is still running. This allows users to always be well-informed about the currently running experiments. In some cases, when an agent's performance is severely lacking, users may want to stop a training session before it has finished, which is enabled through these monitoring tools. We also include the \emph{Live-monitoring} tool in this category, which runs directly after a training session has finished, see \bfref{subsec:LiveMonitoring}.

	\item \textbf{On complete agents} (\bfref{sec:CompleteAgents}): After a training session has finished we have a complete and final set of data available for an agent. This enables us to run more thorough and reliable tests. These can include simulating runs of a marketplace to gather data on the agent's performance in different scenarios and against different competitors, or running a static analysis of the agent's policy for different market states. The tools available for trained agents are in the same way also usable on rule-based agents.
\end{enumerate}

In the following sections, we will take a look at the tools our framework provides for monitoring agents, distinguishing between the two general types of monitoring mentioned above. The goal of these sections is to give a short overview of each tool, how and why they were implemented and what value they offer to the framework as a whole. In \bfref{sec:ImprovingMonitoringTools} we will discuss how the different tools could be improved, explaining how these additions could benefit the entire workflow and enrich the overall experience.

\section{Monitoring during training}\label{sec:DuringTraining}

When talking about monitoring agents during a training session, we are always referring to Reinforcement learning agents, as rule-based agents always perform the same and cannot be trained. But even though they cannot be trained, our monitoring tools listed in the next section, \bfref{sec:CompleteAgents}, can still be applied to rule-based agents as well, as users may want to compare different rule-based strategies against each other or measure the strategy's performance on a market before training an RL agent against it.

Monitoring agents while they are still being trained enables users to be more closely connected to the training process. Ultimately, the goal of such monitoring tools is to be able to predict the estimated `quality' of the final trained agent as reliably as possible while the training is still going.

\subsection{TensorBoard}\label{subsec:TensorBoard}

The \emph{TensorBoard} is an external tool from the RL library \emph{TensorFlow}~\cite{TensorFlow}. With just a few lines of code a training session can be connected to a TensorBoard instance. We are then able to pass any number of parameters and metrics we deem interesting or useful to the TensorBoard, which then offers visualisations for each of them, updating live as the training progresses. In addition to metrics specific to our market simulation, which can be found in \Cref{tab:AllMetrics}, the TensorBoard also visualises a number of specifically training-related data points, such as the number of episodes simulated per second. To access these web-based visualisations, a local server needs to be started. The diagrams created using the TensorBoard are an immensely useful tool for quickly and easily visualising data and offering a first rough comparison of competitors in the market. Aside from simple visualisations, TensorBoard also offers many native plugins and even enables users to write their own~\cite{TensorBoardPlugins}. Plugins such as the \emph{What-If Tool} (\cite{WhatIfTool},~\cite{WhatIfToolWeb}), which allows users to feed trained models with hypothetical situations to study their behaviour, can have a great influence on the way users interact with the TensorBoard and their machine learning models.

\subsection{Live-monitoring}\label{subsec:LiveMonitoring}

Unlike the TensorBoard, the monitoring tools summarised under the term \emph{Live-monitoring} were built by our team, utilising the \emph{Matplotlib}~\cite{Matplotlib} library for visualisation. The \emph{Live-monitoring} tool combines two use cases:

First, it creates visualisations for all data recorded during the training, similar to those provided through the TensorBoard. This needs to be done to be able to quickly access the visualisations after the training has concluded, as the TensorBoard relies on abstract data files and needs to run on a local server in order to create visualisations. By taking the data we have at the end of the training and using our own visualisation tool, we create two types of diagrams: Scatterplots, which contain all samples for a certain parameter (see for example \Cref{fig:SACDuopolyMixedGraphs3}) and lineplots, which show smoothed data, such as it would be available in the TensorBoard (see for example \Cref{fig:SACDuopolyProfitsMean}).

Secondly, the tool simulates a market scenario identical to the one used during training an additional time. To understand why we do this, we need some additional information: during a training session, `intermediate' models, as we will call them, are being saved in regular intervals. These models contain the current policy of the RL agent and can be used the same as any other model of complete agents, the only difference being the quality of the agent, which can change over the course of a training run, both for the better and the worse. These intermediate models can then be used by a range of monitoring tools available to us. Since the models only contain the current policy of an agent, but not the history of states and actions preceding the model, we need to run separate simulations on these models to be able to analyse and evaluate them. For this we utilise our \emph{Agent-monitoring} toolset (explained in detail in \bfref{subsec:AgentMonitoring}). In \bfref{subsec:LiveMonitoringResults} we will discuss the results of a training session using the Live- and Agent-monitoring tools.

\section{Monitoring complete agents}\label{sec:CompleteAgents}

For monitoring trained RL and rule-based agents, we offer three major tools: The Agent-monitoring tool allows users to simulate a large number of episodes to visualise bigger trends, the Exampleprinter simulates a single episode, offering detailed insights into market states using an overview diagram, and the Policyanalyser is a static tool which can be used to analyse a vendor's reaction to different market states and competitor actions.

\subsection{Agent-monitoring}\label{subsec:AgentMonitoring}

\begin{figure}
	\centering
	\includegraphics[height = 0.9\textheight]{images/swimlane_monitoring.pdf}\\
	\caption{The internal workflow when running an Agent-monitoring session. \Cref{tab:AllMetrics} lists the different types of diagrams created by both the Live- and Agent-monitoring and which of the recorded metrics they visualise.}\label{fig:SwimlaneMonitoring}
\end{figure}

The \emph{Agent-monitoring} is a highly configurable tool for monitoring and evaluating different combinations of agents and marketplaces. \Cref{fig:SwimlaneMonitoring} shows how the tool works internally.

In addition to parameters provided to the marketplace and monitored agents, the following parameters can be used to configure the \emph{Agent-monitoring} tool itself:

\medskip
\noindent\textbf{Episodes}: This parameter decides how many independent simulations are run in sequence. At the start of each episode, the market state will be reset and randomised. Within an episode, vendors run through a configurable amount of time steps, during each of which they set prices (depending on the chosen economy type, this can range from only one price for new items to three prices, including a rebuy price for used items) and a set number of customers interact with them.

\medskip
\noindent\textbf{Plot interval}: Some older diagram types enable the user to view averaged or aggregated data over a period of time. The plot interval parameter decides the size of these intervals. Smaller intervals mean more accurate, but also more convoluted data points. Computational time also increases with a smaller interval size.

\medskip
\noindent\textbf{Marketplace}: Using this parameter, the user can set the marketplace on which the monitoring session will be run. Refer to \bfref{sec:MarketScenarios} for an explanation of the different available marketplaces.

\medskip
\noindent\textbf{Separate markets}: This parameter is a boolean flag that determines the way in which the monitoring session will handle the agents given by the user. If the flag is enabled, each agent will be initialised on a separate instance of the chosen marketplace, meaning that the agents will be monitored independent of each other. Running the \emph{Agent-monitoring} with this functionality takes a lot longer than if the flag were disabled, as the whole marketplace is simulated once for each agent. While it may seem like the same results could be reached by simply starting multiple monitoring sessions with a different agent each time, this is not the case. Using this flag instead, it is ensured that all agents get the exact same market states for each episode. By running multiple marketplaces in parallel using the \emph{separate markets} flag, we can match the simulations as closely as possible. The most prominent use case where this flag is enabled is during the Live-monitoring after a training session, where all intermediate models are being monitored on separate markets.

If the flag is disabled, the monitoring tool will initialise only one marketplace and set the passed agents to directly compete against each other on this marketplace. This functionality is most useful when monitoring only a single agent, trying to determine its specific strengths and weaknesses against certain opponents, as it will complete a lot faster than if the flag were enabled.

\medskip
\noindent\textbf{Agents}: Depending on the chosen marketplace, only a select number of agents can be chosen to be monitored, as each agent is built to interact with a specific type of marketplace. First off, all agents belong to one of the two major categories: \emph{Reinforcement learning agent} or \emph{rule-based agent} (for a more detailed overview see \bfref{sec:ExplainVendors}). RL agents can only be monitored on the marketplace type and market environment they were trained on, as these define the number of inputs and outputs the agent expects. Rule-based agents can only be used on the marketplace type they were built for, as each of them makes assumptions about the number of prices they will need to set, but the market environment may be freely chosen. This leads to not all marketplace types having the same amount of rule-based vendors available. Following their importance for our simulation framework, the Linear Economy has the least and most often weakest vendors available, while the more refined competitors are most of the times only available as a version compatible with the Circular Economy with rebuy prices.

\medskip
During each episode and for each vendor, all market events are being recorded. At the end of the monitoring session, the collected data is evaluated in different visual formats. First of all, all data that would be available to see in the \emph{TensorBoard} during a training session is visualised using density plots. These plots can be used to compare the vendors in a more granular way, if for example the effect of a parameter on the customer's sell-back behaviour of used items should be tested. Another visualisation that is created is a histogram containing the cumulative profits per episode for each agent, plotted against each other (see for example \Cref{fig:PPOOligopolyCumulativeRewards}). This allows for a quick overview to see which agent had an overall better performance. One additional type of diagram is only created if the \emph{Agent-monitoring} is run through the Live-monitoring tool: Violinplots. These plots, which are created for all data points available through TensorBoard (also see \Cref{tab:AllMetrics}), depict distributions using density curves, accentuating the minimum, maximum and median values. Violinplots are used by us to compare different training stages of an agent, as small policy changes can have great impact on these values. For exemplary Violinplots created after training, see \Cref{fig:SACDuopolyViolinPlots}.

Aside from monitoring after a training session, a common use case of the \emph{Agent-monitoring} tool is to test trained agents against competitors other than the ones they were trained against. This is done to test an agent's capacity to adapt to different circumstances, an important factor in deciding its quality, as its competitors in the real market will differ from any it has encountered in training, due to the sheer vastness of options when it comes to dynamic pricing models available nowadays. %, see \bfref{sec:ExplainVendors}.

\subsection{Exampleprinter}\label{subsec:Exampleprinter}

The \emph{Exampleprinter} is a tool meant for quickly evaluating a market scenario in-depth. When run, each action taken by the monitored agents is being recorded, in addition to market states and events such as the number of customers arriving and the amount of products thrown away. At the end of this quick simulation, an animated overview diagram is created, which shows all actions and their consequences for each step in the simulation, see for example \Cref{fig:SACDuopolyExampleprinter17}. Due to the large amount of data that is being collected and visualised and the overhead would come with doing so for hundreds of episodes, we chose to disconnect this functionality from large-scale tools such as the Agent-monitoring. While the Agent-monitoring could be seen as a tool that imitates Macro-economic behaviour, simulating hundreds of days through hundreds of episodes, the \emph{Exampleprinter} instead simulates only one day, recording and visualising all data collected during that time.

\subsection{Policyanalyser}\label{subsec:Policyanalyser}

The last tool we want to introduce is the \emph{Policyanalyser}. The \emph{Policyanalyser} is our only tool which does not simulate a market. Instead, the tool can be used to monitor an agent's reaction to different market states. The user can decide on up to two different features to give as an input, such as a competitor's new and refurbished prices, and the \emph{Policyanalyser} will feed all possible input combinations to the agent and record its reactions. When initialising the \emph{Policyanalyser}, the user defines a number of parameters: The agent whose policy should be analysed, as well as the marketplace and the competitors that should be used, just as is done for all the other tools. Additionally, the user defines a \emph{template market state}, a market state containing all values that are passed to the analysed agent, such as the number of items currently in circulation and the prices of competitors. Lastly, a list of \emph{analysed features} needs to be provided, which defines one or two features of the template market state that should be varied. When the \emph{Policyanalyser} is run, these features are inserted into the template market state, overwriting the initial values and creating a new combination. This new market state is then passed on to the \emph{policy}-method of the analysed agent (example policies of some of our rule-based agents can be found in \bfref{sec:AppendixPolicies}), and its reactions are recorded and visualised. See \bfref{subsec:ResultsPolicyanalyser} for use cases of this tool.

The \emph{Policyanalyser} is the monitoring tool which operates on the smallest scale out of all the tools we built for our framework. It allows users to define any market state they want and to then accurately monitor a vendor's reactions to changes to this specific state. While the tool can just as well be utilised to test new rule-based strategies, it is very much meant to be used as a way to understand RL agents better, as their policies are not immediately visible to the user and must therefore be discovered through tools such as the one's we built.
