\begin{jointwork}\label{ch:Approaches}\todo{Alex: `Klassendiagramm/FMC-Diagramm für die Übersicht? Du nennst recht Häufig Komponenten, eine Gesamtübersicht wie sie interagieren wäre gar nicht schlecht.' -> Würde ich in die introduction packen}
	In this section we will take a look at the different approaches we took to monitoring agents in our framework, explaining the reasons why we chose to implement specific features and how they help us in determining an agents strengths and weaknesses.
\end{jointwork}

\section{When to monitor what}\label{sec:WhenToMonitorWhat}

Our \emph{workflow} (which will be explained in more detail in \nameref{ch:OurWorkflow}) can generally be split into two parts when it comes to monitoring and evaluating agents: \emph{during} and \emph{after} training. When talking about the \emph{workflow} we refer to the process of configuring and starting a training session, where a Reinforcement-Learning agent is being trained on a specific marketplace against competitors. The \emph{workflow} also includes the subsequent collection of data used to evaluate the agent's performance. We are also introducing the term of the \emph{complete agent} in this section, which will be used to refer to both Reinforcement-Learning agents that have been fully trained as well as rule-based agents which do not need training.

As mentioned above, we split our monitoring tools into the following two categories:

\begin{enumerate}
	\item During training: Having data available as soon as possible without having to wait for a long training session to end is crucial to an efficient workflow. Our framework enables us to collect, visualize and analyze data while a training session is still running.\todo{Implement this feature. It should at least be able to temporarily halt the training to start an intermediate monitoring session} This enables us to filter out agents with sub-par performance prematurely. This can be done using user-defined threshholds or on the basis of previoulsly collected data (e.g. only keeping agents that are performing better than at least 50\% of other agents after the same amount of training in comparable scenarios.) \todo{Also implement this feature. Allow users to set rules for when a training session should be terminated if the agent is not performing well at a certain point. Also, it should be able to give the program a set of datapoints and have it set rules if possible}

	\item On complete agents: After a training session has finished we have a complete and final set of data available for an agent, which enables us to perform more thorough and reliable tests. These can include simulating runs of a marketplace to gather data on the agent's performance in different scenarios and against different competitors, or running a static analysis of the agent's policy in different market states. The tools available for trained agents are in the same way also usable on rule-based agents.
\end{enumerate}

In \todo{If we also conduct experiments in this chapter, mention it here}the following sections, we will take a look at all of the tools our framework provides for monitoring agents, distinguishing between the two general types of monitoring mentioned above. The goal of this section is to give a short overview of each tool, how and why they were implemented and what value they offer to the framework as a whole and to the analysis of agent reliability and robustness in particular. We will also discuss features that are currently not available, explaining how they could benefit the entire workflow or enrich the overall experience.

\section{Monitoring during a training session}\todo{Alex: `Der erste Satz in 4.2 ist nicht nur keine Footnote, sondern durchaus wichtig, mir wurde aus Kap 3 nicht unbedingt klar, welche der Pricing-Systeme ausgemessen werden. Bei nicht-lernenden Strategien ließen sich ja durchaus Performance-Kriterien ableiten, die in der Praxis ausgemessen werden müssten (zB wenn man wissen will wie 2 unterschiedlich konfigurierte Rule-Based Agents miteinander interagieren oder welche Rand-Szenarien dazu führen dass eine rule-based Strategie `exploitet' wird).'}

When talking about monitoring agents during a training session, we are always talking about Reinforcement-Learning agents, since Rule-Based agents always perform the same and cannot be trained. Monitoring agents while they are still being trained enables us to be more closely connected to the training process. Ultimately, the goal of such monitoring tools is to be able to predict the estimated `quality' of the final trained agent as reliably as possible while the training is still going. Users must however be careful when interpreting the results of monitoring tools that work on `incomplete' agents, we will go more into this  in \nameref{ch:InterpretResults}.

\subsection*{TensorBoard}\label{subsec:TensorBoard}

The \emph{TensorBoard} is an external tool from the from the Reinforcement-Learning library \emph{TensorFlow}\footnote[0][-0.2]{https://www.tensorflow.org/}\todo{Citation or footnote?}. With just a few lines of code \todo{Question for the tutors: Do I give an example of such code?} a training session can be connected to a TensorBoard. We are then able to pass any number of parameters and metrics we deem interesting or useful to the TensorBoard, which then offers visualizations for each of them, updating live as the training progresses. Though the TensorBoard does not interpret data in any way, it is an immensely useful tool for quickly and easily recording data and offering a first rough comparison of competitors in the market.
\todo{Create some sample diagrams. Perhaps these can be re-used in the workflow section, so they could perhaps be moved to the Appendix for reference?}

\subsection*{Live-monitoring}\label{subsec:LiveMonitoring}

Unlike the TensorBoard, the monitoring tools summarised under the term \emph{live-monitoring} were completely and from the ground up built by our team. For most of the visualizations, the \emph{matplotlib}\footnote[0][-0.2]{https://matplotlib.org/stable/index.html} library was used. Live-monitoring aims to be more configurable and in-depth with the metrics it offers than the aforementioned TensorBoard. \todo{make the current live-monitoring so that graphs are actually being created while the training is running, not like it is now with graphs only being created afterwards} During a training session, `intermediate' models, as we will call them, are being saved after a set number of episodes. \todo{Have we introduced the concept of episodes before? Should a Glossary be introduced, or do we do this stuff in the introduction?} These models contain the current policy of the agent and can be used the same as models of complete agents, the only difference being the quality of the agent, as those with a lower amount of trained episodes generally perform worse. \todo{Alex: `das ist eine Behauptung, die du definitiv untermauern musst. Gerade RL Agenten neigen zu Catastrophic Forgetting, was deiner These hier diametral wiederspricht. Wenn das bei eurem Use-Case nicht auftritt, wäre es hier an der Stelle den empirischen Beweis dafür aus euren Messungen wenigstens zu nennen, sollte er später noch folgen.'} These intermediate models can then be used by a range of monitoring tools available to us. Since the models only contain the current policy of an agent but not the history of states and actions preceding the model, we need to run separate simulations on these models to be able to analyze and evaluate them. For this, we utilize our \emph{agent-monitoring} toolset, which will be explained in more detail in the \nameref{subsec:AgentMonitoring} section.

\section{Monitoring complete agents}\label{sec:CompleteAgents}

The following tools offer a wide range of functionalities for monitoring complete agents. We can use these tools to both determine the reliability and robustness of one certain agent, but also to compare different agents, either during a joined monitoring session or by comparing results of monitoring runs on identically parametrized market situations.

\subsection*{Agent-monitoring}\label{subsec:AgentMonitoring}

Using the \emph{Agent-monitoring} toolset, users can configure a custom market simulation, using the following parameters:

\begin{enumerate}
	\item Episodes: This parameter decides how many independent simulations are run in sequence. At the start of each episode, the market state will be reset. Within an episode, each vendor runs through 50 timesteps, during each of which a price is set (depending on the chosen economy type, this can range from only one price for new items to three prices, including a re-buy price for used items) and a set number of customers interact with the vendors.
	\item Plot interval: A number of diagram types enable the user to view averaged or aggregated data over a period of time. The plot interval parameter decides the size of these intervals. Smaller intervals mean more accurate but also more convoluted data points. Computational time also increases with a smaller interval size.\todo{These diagrams can be seen/are explained in...}
	\item Marketplace: Using this parameter, the user can set the marketplace on which the monitoring session will be run. See \nameref{sec:MarketScenarios} for an explanation of the different available marketplaces.
	\item Separate marketplaces: This parameter is a boolean flag that determines the way in which the monitoring session will handle the agents given by the user. If the flag is enabled, each agent will be initialized on a separate instance of the chosen marketplace, meaning that the agents will be monitored independently from each other. In this case, each agent will play against a pre-defined set of competitors, depending on the chosen marketplace (see also \nameref{sec:MarketScenarios}). This functionality is most useful when directly comparing two or more agents against each other, as all of them will be playing under the same circumstances, without influencing each other. While it may seem like the same results could be reahced by simply starting multiple monitoring sessions with a different agent each, this is not the case. Using this flag instead, it is ensured that all agents get the exact same starting conditions for each episode. Normally, the market state is randomly shuffled at the beginning of each episode, which would lead to inconsistent results. By running multiple marketplaces in parallel using the \emph{Agent-monitoring} tool, we can ensure that all agents are given the same states at the start of an episode, to match the simulations as closely as possible. However, we cannot influence the competitors' behaviour in the different marketplaces, as these are always dependent on the agents' actions and can therefore not be replicated on different markets.

	If the flag is disabled, the monitoring tool will initialize only one marketplace and set the passed agents to directly compete against each other on this marketplace. In this case, the number of agents that must be passed is determined by the number of allowed competing vendors defined by the market environment. This functionality is most useful when evaluating a single agent, trying to determine its specific strengths and weaknesses against certain opponents, something that would not be possible with the static pre-set competitors. In this sense, the configuration can be used in two ways: To either have multiple trained agents play against each other to find out which one beats the other, or to monitor a single agent against a custom selection of rule-based competitors not available otherwise. In order to confirm trends seen during the training process, the same configuration can be re-used for this tool to have the trained agent play against the same competitors on the same market again. Similarly, micro-adjustments can be made to analyze how the trained agent reacts to changes to his learnt environment.
	\item Agents: Depending on the chosen marketplace, only a select number of valid agents can be chosen to be monitored, as each agent is built to interact with a specific type of marketplace. First off, all agents belong to one of the two major categories: \emph{Reinforcement Learning agent} or \emph{Rulebased agent} (for a more detailed overview see \nameref{sec:ExplainVendors}). Reinforcement Learning agents can only be monitored on the marketplace type they were trained on, while the market environment may be changed. Rulebased agents can similarly only be used on the marketplace type they were built for, as each of them makes assumptions about the number of prices they will need to set. This leads to not all marketplace types having the same amount of rulebased vendors available. Following their importance for our simulation framework, the Linear Economy has the least and most often weakest vendors available, while the more refined competitors are most of the times only available as a version compatible with the Circular Economy with rebuy prices.
\end{enumerate}

% It is important to note that while multiple agents are being monitored simultaneously, each agent operates on a market of its own. This means that while each agent receives the same initial market state at the start of an episode, and all agents play against the same competitors, monitored agents do not play against each other. The goal of the \emph{Agent-monitoring} tool is to be able to compare different agents playing under the same circumstances to determine which ones might be `better' than others. This would not be achievable with monitored agents playing against each other, as the assumptions about the market state would differ for each agent.

During each episode and for each vendor, all actions and market events are being recorded using \emph{watchers}.\todo{Refer to an explanation in the workflow chapter} At the end of a monitoring session, the collected data is evaluated in different visual formats. First of all, all data that would be available to see in the \emph{TensorBoard} during a training session is visualized using density plots. These plots can be used to compare the vendors in a more granular way, if for example the effect of a parameter on the customer's sell-back behaviour of used items should be tested. Another view that is created is a bar diagram containing the cumulative profits per episode for each agent plotted against each other. This allows for a quick overview to see which agent had an overall better performance.

The most common usecase of the \emph{Agent-monitoring} tool is to test trained agents against competitors other than the ones it was trained against. This is done to test the \emph{Reliability} of the agent, an important factor in deciding an agent's quality, as its competitors in the real market will differ from any it has encountered in training, due to the sheer vastness of options when it comes to dynamic pricing models available nowadays, see \nameref{sec:ExplainVendors}.

\subsection*{Exampleprinter}

The \emph{Exampleprinter} is a tool meant for quickly evaluating an agent in-depth. When run, each action the monitored agent takes is being recorded, in addition to market states and events, such as the number of customers arriving and the amount of products thrown away. For certain market scenarios such as the \emph{CircularEconomyRebuyPriceDuopoly}, which simulates a circular economy model with rebuy prices in which two vendors compete against each other, these actions and events are also being summarised as an animated graphic, where each time-step is being illustrated. \todo{Add in a graphic here. Perhaps have two of the time-steps to `simulate' the animation}

\subsection*{Policyanalyzer}

The \emph{Policyanalyzer} is our only tool which does not simulate a run of the market scenario. Instead, the tool can be used to monitor an agent's reaction to different market events. The user can decide on up to two different features to give as an input, such as the competitor's new and used prices, and the Policyanalyzer will feed all possible input combinations to the agent and record its reactions. \todo{Diagram}

\section{Features for the future}

This section is meant as a collection of ideas and approaches for tools that could further enhance the workflow, but which have not been implemented and therefore not been tested for their feasability and usefulness.