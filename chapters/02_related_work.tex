\begin{jointwork}\label{ch:RelatedWork}
	This section will outline approaches to modelling and simulating recommerce marketplaces, as well as give an introduction into the concepts of Reinforcement-Learning, the technology the framework is built for. Additionally, novel and unconventional approaches to monitoring and evaluation of these Reinforcement-Learning agents will be presented.
\end{jointwork}

\section*{Dynamic Pricing simulations}

The topic of dynamic pricing techniques is well explored, with the earliest mathematical models having been developed over a century ago \cite{DynamicPricingHistory}. With the emergence of e-commerce and the ability to cheaply and quickly change prices, as well as the freedom of information, including being able to know competitor's prices in real time, the topic has gained importance even further. Dynamic pricing methods come in many different shapes and forms, from simple greedy algorithms over customer-choice methodology to machine-learning models \cite{deGeerPricing}. Due to the sheer number of available options when attempting to choose a pricing method to use, and to be able to evaluate the relative performance of new algorithms, a unified platform for comparison is needed. The real market is not really an option for this, as it is near impossible to create equal opportunities for each pricing method to be able to reliably compare them to each other. This is one of the major reasons for simulating the real market - development, testing and comparison of new pricing methods.

Nowadays, another market dimension is creating new challenges for businesses: Circular Economy models (see \nameref{sec:CircularEconomy}) which include two or more prices per product make it hard to price items manually using pricing experts or even regular dynamic pricing models, which may not be able to correctly adapt to interdependencies between prices. This is creating a demand for dynamic pricing methods using self-learning technologies, such as Reinforcement-Learning. And again, to train, test and evaluate such pricing agents, simulated environments are needed. In the case of Reinforcement-Learning, this is not only optional but required, as Reinforcement-Learning agents need to learn how a market environment works and how to interact with it before they become viable for use on the real market, as the pricing decision they take early on in the training process lack any experience and are therefore taken randomly, until enough knowledge about the market was collected.

\section*{Visualization - State-of-the-art}

The process of training Reinforcement-Learning agents for any kind of task always comes with the requirement for visualizing the data collected during training. This allows for an analysis of the algorithm's performance and gives insights into its strengths and weaknesses.

For the past years, going back as far as 2018, one of the most used frameworks overall and the most used for machine learning was TensorFlow~\cite{StackOverflowSurvey}. Aside from its API for model building, TensorFlow also provides a visualization toolkit called TensorBoard, which can be used independent of other TensorFlow tools. TensorBoard provides an API for tracking and visualizing important metrics such as loss and accuracy, and allows developers to easily integrate their own metrics as well. During an experiment, data can be visualized live during the training process, allowing developers to quickly gain insights into the performance of the algorithms. In our recommerce market simulation framework, we use the TensorBoard in conjunction with our own tools for data visualization, see \nameref{ch:Approaches}\todo{Do I refer to our own project in the related work chapter?}\todo{Do I need more references for TensorBoard, or some pictures?}.

\section*{Visualization - Novel approaches}

\subsection*{Model visualization}

Aside from visualizing results of a training run, developers may also want to visualize the model itself. Tools such as the \emph{Graph Visualizer}~\cite{GraphVisualizer} are a step in the right direction, but the study found that while developers are satisfied with the visualization tool itself, they would prefer to be able to edit and influence the graph model directly. The \emph{SimTF} tool developed by Leung et al.~\cite{NeuralNetworkVisualization} attempts to solve this problem with. The same as the \emph{Graph Visualizer}, the \emph{SimTF} tool is based on TensorFlow. The authors describe it as a \emph{library for neural network model building}. \emph{SimTF} acts as a middleman between the visually constructed neural network graph model and the TensorFlow API, allowing developers to modify the visualized model to influence the training network.

In our simulation framework, the neural network model itself is quite static, users can only choose from a pre-set selection of network sizes, and all changes to hyperparameters must be done through configuration files ahead of a training run.

\subsection*{Evaluation}

On the other side of the visualization tools we have those which provide insights into currently running or completed training runs, such as the previously mentioned \emph{TensorBoard}. Besides simply visualizing different collected data, the goal of most of these tools is to allow the developers to get a sense of the trained algorithm's performance, and to evaluate and compare it against previously trained agents using the same algorithm or other algorithm's completely.

When developing new algorithms, the mnost common barrier to proper comparison with current state-of-the-art algorithms is a lack in reproducibility of results. Benchmark environments such as those provided by OpenAI Gym \cite{OpenAIGym} lower this barrier by providing unified interfaces against which many algorithms have already been tested. However, the effects of extrinsic factors (such as hyperparameters) and intrinsic factors (such as random seeds for environment initialization) make proper, reliable reproducability a challenge, something for which current evaluation practices do not account for~\cite{DRLThatMatters}. Jordan et al.~\cite{EvaluatingPerformance} propose a new, \emph{comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance}. The authors describe the goal of their new method as not trying to find methods (algorithms) that maximize performance with optimal hyperparameters, but rather to find those that do not require extensive hyperparameter tuning and can therefore easily be applied to new problems. This leads to a preference for algorithms that are not aimed at being the best at problems which are already solved (which applies to the aforementioned benchmark environments), but instead those which are most likely to succeed in new, unexplored environments.