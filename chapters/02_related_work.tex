\begin{jointwork}\label{ch:RelatedWork}
	This chapter introduces the history and importance of dynamic pricing methods and their connection to the demand for simulated marketplaces. Additionally, traditional and novel approaches to monitoring and evaluating Reinforcement learning agents will be presented.
\end{jointwork}

\section{Dynamic Pricing}

The topic of dynamic pricing techniques is well explored, with the earliest mathematical models having been developed over a century ago \cite{DynamicPricingHistory}. With the emergence of e-commerce and the ability to cheaply and quickly change prices, as well as increased freedom of information, including being able to know competitors' prices in real time, the topic has gained importance even further. However, research concerning dynamic pricing in e-commerce, especially in highly competitive markets, has not grown at the same pace as the markets themselves~\cite{PricingEcommerceGrowth}. On the other hand, the role that autonomous agents will play in this new market environment, in the form of vendors and customers alike, has for a long time been a topic of discussion and speculation~\cite{PricingBySoftwareAgents}. Dynamic pricing methods come in many different shapes and forms, from simple greedy algorithms over customer-choice methodology to machine learning models \cite{deGeerPricing}. Due to this high number of options to choose from, and to enable researchers to evaluate their algorithms' performance, a unified platform for comparison is needed. The real market is not really an option for this, as it is nearly impossible to create equal opportunities for each pricing agent to be able to reliably compare them to each other. This is one of the major reasons for simulating the real market - development, testing and comparison of new pricing methods.

% Nowadays, another market dimension is creating new challenges for businesses: Circular Economy models (see \bfref{sec:CircularEconomy}) which include two or more prices per product make it hard to price items manually using pricing experts or even regular dynamic pricing models, which may not be able to correctly adapt to interdependencies between prices. This is creating a demand for dynamic pricing methods using self-learning technologies, such as RL. And again, to train, test and evaluate such pricing agents, simulated environments are needed. In the case of RL, this is not only optional but required, as RL agents need to learn how a market environment works and how to interact with it before they become viable for use on the real market, as the pricing decision they take early on in the training process lack any experience and are therefore taken randomly, until enough knowledge about the market was collected.\todo{Judith: `Diesen letzten Absatz verstehe ich nicht so ganz. Hast Du das mit der CE nicht in dem 1.2 Genauso erklärt? Dann könntest Du hier den ersten Satz des Absatzes deutlich kürzen, bzw. Den kannst Du sowieso nochmal Teilen. Warum motivierst Du hier RL? Würde das nicht besser in das RL Kapitel passen? '}

\section{Visualisation - State-of-the-art}

% The process of training Reinforcement learning agents for any kind of task comes with the requirement for visualising the data collected during training. This allows for an analysis of an algorithm's performance and gives insights into its strengths and weaknesses.
When training RL agents, it is almost a requirement to be able to visualise data collected during training, to allow for an analysis of the algorithm's performance.

For the past years, going back as far as 2018, one of the most used programming frameworks overall and the most used for machine learning was TensorFlow~\cite{StackOverflowSurvey}. Aside from its API for model building, TensorFlow also provides a front-end visualisation toolkit called TensorBoard, which can be used independent of other TensorFlow tools. TensorBoard provides an API for tracking and visualising important metrics such as loss and accuracy of a trained model, and allows developers to easily integrate their own metrics as well. During an experiment, data can be visualised live during the training process, allowing developers to quickly gain insights into the performance of the algorithms. In our market simulation framework, we use the TensorBoard in conjunction with our own tools for data visualisation, see \cbfref{ch:Approaches}.

\section{Visualisation - Novel approaches}

\subsection{Model visualisation}

Aside from visualising results of a training run, developers may also want to visualise the model itself. Tools such as the \emph{Graph Visualizer}~\cite{GraphVisualizer} are a step in the right direction, but the authors found that while developers are satisfied with the visualisation tool itself, they would prefer to be able to edit and influence the graph model directly. The \emph{SimTF} tool developed by Leung et al.~\cite{NeuralNetworkVisualization} attempts to solve this problem. The same as the \emph{Graph Visualizer}, the \emph{SimTF} tool is based on TensorFlow. The authors describe it as a \emph{library for neural network model building}. \emph{SimTF} acts as a middleman between the visually constructed neural network graph model and the TensorFlow API, allowing developers to modify the visualised model to influence the training network.

In our simulation framework, the used neural network model itself is quite static, as users can only choose from a pre-set selection of network sizes, and all changes to hyperparameters must be done through configuration files ahead of a training run.

\subsection{Evaluation}

On the other side of the visualisation tools we have those which provide insights into currently running or completed training runs, such as the previously mentioned \emph{TensorBoard}. Besides simply visualising different collected metrics, the goal of most of these tools is to allow the developers to get a sense of the trained algorithm's performance, and to evaluate and compare it against previously trained agents using the same algorithm or other algorithm's completely.

When developing new algorithms, the most common barrier to proper comparison with current state-of-the-art algorithms is a lack in reproducibility of results. Benchmark environments, such as those provided by OpenAI Gym \cite{OpenAIGym}, lower this barrier by providing unified interfaces against which many algorithms have already been tested. However, the effects of extrinsic factors (such as hyperparameters) and intrinsic factors (such as random seeds for environment initialisation) make proper, reliable reproducibility a challenge, something for which current evaluation practices do not account for~\cite{DRLThatMatters}. Jordan et al.~\cite{EvaluatingPerformance} propose a new, \emph{`comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance'}. The authors describe the goal of their new evaluation technique as not trying to find methods (algorithms) that maximise performance with optimal hyperparameters, but rather to find those that do not require extensive hyperparameter tuning and can therefore easily be applied to new problems. This leads to a preference for algorithms that are not aimed at being the best at problems which are already solved (which applies to the aforementioned benchmark environments), but instead those which are most likely to succeed in new, unexplored environments.

In our framework, we attempt to provide users with visualisations for as many different metrics as possible, to enable them to properly gauge the fitness of the currently tested dynamic pricing method.