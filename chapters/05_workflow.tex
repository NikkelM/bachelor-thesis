\begin{jointwork}\label{ch:OurWorkflow}
	The main goal of the market simulation framework is to provide a simple-to-use but powerful tool for training Reinforcement-Learning algorithms on highly configurable markets for users in both a research and a business context. To achieve this, multiple components had to be developed and connected to create the workflow we now provide. This section will introduce the most important parts of the workflow.
\end{jointwork}
\todo{Judith: `Maybe change chapter order of this with Chapter 4?'}

When working with our simulation framework, users can choose from two options: First, it is possible to use our tool via a custom Command line interface (CLI). Alternatively, users can interact with the framework through a web-interface, which utilizes Docker for remote-deployment of tasks issued by the user. For detailed insights into our web-interface and remote deployment processes, see \cite{JudithThesis}.

\Cref{fig:WorkflowSwimlanes} depicts the common workflow activities in our framework. For all possible tasks, the user needs to provide configuration files, which define both the task to be worked on as well as parameters which influence market and agent behaviour (see \nameref{sec:ConfigureRun}). After the configuration files have been validated, the simulation framework initialized the requested marketplace and agents, and then starts the requested task, for which there are currently three options provided through the CLI or web-interface:

\begin{itemize}
	\item Training
	\item (Agent-)Monitoring
	\item Exampleprinter
\end{itemize}

At the end of the respective task, the user is always provided with the various diagrams and data collected during the respective task, which can then be used to start subsequent tasks.

\section{Configuring the run}\label{sec:ConfigureRun}

\begin{figure}[t]
	\centering
	\includegraphics[width = \textwidth]{images/workflow_swimlanes.pdf}\\
	\caption{Swimlane diagram depicting possible workflows, possible webserver interaction omitted}\label{fig:WorkflowSwimlanes}
\end{figure}

Configuration is one of the most important aspects of the workflow. Without it, each simulation and training session would produce similar, if not the same results. By tweaking different parameters of a run, market dynamics can be changed and agent behaviour and thereby performance be influenced. The goal of our monitoring tools is to enable users to assess the extent to which each parameter influences certain characteristics of the training and/or monitoring session, and to enable them to make informed decisions for subsequent experiments.

Ultimately, all configuration is done using various \texttt{.json} files which contain key-value pairs of the different configurable items. We further differentiate between different groups of configurations, which means that hyperparameters influencing the market, such as maximum possible prices or storage costs, are being handled separate from parameters needed for Reinforcement-Learning Agents, such as their learning rates, allowing users to easily change and tweak parameters involving different parts of the framework. Examples of such configuration files can be found in \Cref{fig:SACDuopolyConfigEnvironment} and \Cref{fig:SACDuopolyConfigMarket}.

% Our main goals when building our configuration tools were to make sure that users are able to quickly and safely configure their experiments in a way that is also easily reproducible. To be as user-friendly as possible, a lot of validation logic has been implemented to be able to catch invalid configurations as early as possible, making sure that whenever a training session is started, it is confirmed that the configuration is valid and the training will not fail due to wrongly set parameters at any point. Since our project has also been deployed to a remotely accessible, high-performant machine, we decided on creating an approachable web-interface for our configuration tools, which can now be used for both configuring, starting and evaluating experiments.

% \subsection*{The webserver/Docker-API}\todo{Judith: `Das ganze wie nutze ich die Konfiguration auf dem Webserver könntest Du meiner Meinung nach kürzer fassen'}

% \todo{Example screenshot of the webserver. Configuration and running page?}
% On our webserver, users can upload .json files containing any number of (valid) key-value pairs, or create configurations using a self-validating form, always making sure that it contains only valid values. For example, if the user chooses a Circular Economy market scenario with re-buy prices enabled, the user will not be able to configure agents that cannot operate on such a market.

% Aside from uploading or configuring complete, ready-to-go configurations, users can also choose to upload or create partial configurations, which can then be combined with other configuration objects to create a complete, valid configuration. This allows users to create multiple incomplete configurations containing only select parameters and then test all permutations of these configurations to observe the effect the different parameter combinations have on the experiment. \todo{Mention that this currently needs to be done manually, future work section?}

% An example of one such approach can be found below:
% \todo{Example of a mix and match of parameters}

% After having configured the experiment, the webserver also offers the option of starting multiple runs of the same configuration simultaneously. Most of the times this is recommended, as singular runs are prone to misleading results due to the trained agents training with specific market states, which in some cases may lead to behaviour unique to this starting configuration\todo{Example: Two completely different runs with the same configuration}. By running the same configuration multiple times using different starting circumstances for both the agent that is to be trained as well as its competitors, a configuration's effect on the agent's performance can be disconnected from the pseudo-random market states which influence the vendor's decision making processes.

%% This is regarding Reliability. Maybe I can re-use this in the Interpretation-Chapter, but since I removed it from the title/thesis focus, it is sort of too much for this section
% \todo{Is this perhaps too much interpretation for this section?}\todo{Alex: `Zu deiner Anmerkung: Du hast 2 Optionen um so etwas zu betrachten, entweder Mean + Standardabweichung oder Median + Quantile/Dezile bei sehr stark asymmetrischen Verteilungen. Was für deinen Use-Case besser ist kann ich nicht ausm Stehgreif sagen ohne die Daten gesehen zu haben'} A configuration can be interpreted as producing `reliable' agents if any number of simulations all converge on similar agent performances (meaning similar mean rewards). The more singular runs diverge from the mean, \todo{mean or median? What do I use?}the more the agent's performance is dependent on the initial market state, meaning that its performance is less stable, as it will produce unpredictable results on new, unknown market scenarios. A higher number of runs that produce agents which perform on similar levels of performance means that the specific configuration, to be exact the parameters responsible for the Reinforcement-Learning algorithm, produces `reliable' agents: No matter the state of the market, the agent can always be expected to perform on the same level that was observed during the training process, setting prices that lead to similar profits. Reliability is therefore one of the most important factors in determing an agent's overall quality, as the ultimate goal is always to be able to deploy the agent onto the real market, which will always differ from the simulated environments the agent experienced during training, simply due to the fact that real markets are being influenced by so many more parameters than one could model in our, or for that matter any simulation framework. It should be noted that the chosen Reinforcement-Learning algorithm drives the reliability of a configuration, as many of the other hyperparameters most often have no impact on this aspect of an experiments results.

%% This section is very long but doesn't really add any useful information
% \section{Choosing what to show the user during training}\todo{Alex: `Die Struktur von 5.2 ist mir noch nicht ganz klar, aber da hört dein Draft ja auch auf, insofern ist das wohl harmlos'}

% During the training process, we of course record all actions the different vendors take, both for the training agent and its competitors, be they Reinforcement-Learning agents as well, or Rule-Based agents. Market states and events are also being recorded, to be able to match them to the agent's actions later on. Users of course want to have an indicator of how the training run is going so far, both to inform themselves about the total progress of the training, but also about the quality of the agent they are training. We now have to decide what information is shown to the user, making sure that the user is well informed, whilst also holding back on displaying too much information at once to prevent confusion, as the underlying data and states are ever-evolving, leading to many datapoints being invalidated shortly after their creation. This potential overflow of information is the reason why we decided on displaying minimal information to the user while the training session is still running: Training progress and current performance, in the form of the current overall mean profit.

% The first choice and a save bet when looking at datapoints that will be shown to users is the current training progress. Each training run is defined in its length by a parameter that sets the number of episodes that should be simulated. Each episode consists of an independently initialized market state on which a configurable amount of steps is run. Within each step, the vendors take turns in observing the current market state and then setting their prices for the new and used products and depending on the chosen marketplace, a rebuy price.\todo{Should episodes and steps be defined/explained earlier than this? In the introduction? Specific section/Chapter for important terms and phrases?} We use the training progress to drive the frequency with which data is being shown to the user using the console: Every 10 episodes, we output the current progress, containing both the current number of steps and episodes completed, as well as the mean reward over all episodes the agent has completed. Additionally, to give a more visual component to the training progress a progress bar is displayed, which also tries to estimate the training-time remaining, based on the previous length the simulation of an episode took.

% Using only these two datapoints, the user knows the two most important things: How far along in the training process he is, and how the agent is currently performing.

% \todo{Small paragraph for rl-vs-rl and self-training having different output}

\section{The monitoring workflow}

Within the recommerce workflow, there are two points in time where our monitoring tools are or can be used. While a training session is running, the \nameref{subsec:TensorBoard} tool is automatically used to record metrics and give insights into the current training run, by creating visualizations of current states and actions. By saving intermediate models, the \nameref{subsec:LiveMonitoring} enables the user to compare agent models saved at different times within the training process. This can be especially useful when trying to determine the most effective amount of training steps after which a model does not improve further, or even worse, starts to deteriorate in performance\todo{Citation for the `possibility' of this}, to optimize future runs. After the training has finished, or at any point disconnected from a training session, our tools described in \nameref{sec:CompleteAgents} can be used to further analyze the trained models.\todo{Judith: `Auch wenn ich das training abbreche?' -> Mention this in chapter 6, run a long experiment and stop it prematurely. Live-monitoring is not run, but models are still saved}

\subsection*{After training}

After training has completed, the most crucial phase of the workflow starts\todo{This sentence is a bit harsh. Make it less `Crucial'}. As has been mentioned before, the end-goal of users using the simulation framework is to be able to deploy trained agents into real markets to set prices independently of human inputs or guidance. Starting from this premise, the goal of the training process is therefore to model the marketplace as realistically as possible, so that the trained agents can transfer the knowledge from their training to the real market `without noticing a difference'. Before deploying agents to the real market, they need to be tested on their reliability and robustness, to make sure that their policies are not just effective under certain circumstances, but that they are also able to adapt to different market scenarios and behave accordingly.