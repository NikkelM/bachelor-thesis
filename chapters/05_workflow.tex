\label{chapter:OurWorkflow}

\begin{jointwork}
	The main goal of the bachelor's project is to provide a simple-to-use but powerful interface for training Reinforcement-Learning algorithms on highly configurable markets for users in both a research and a business context. To achieve this, multiple components \todo{Better word for components}had to be developed and connected to create the workflow we now provide. This section will go over the most important parts of the workflow, focusing on the way each of them supports the monitoring capabilities of the framework.
\end{jointwork}

\section{Configuring the run}

Unarguably\todo{Is this "zu wertend"?}, the most important part of the whole workflow is its configuration. Without it, each simulation and training session would produce similar, if not the same results. By tweaking different parameters of a run, market dynamics can be changed and agent performance be influenced. The goal of our monitoring tools is to enable users to assess the extent to which each parameter influences certain characteristics of the training and/or monitoring session, and to enable them to make informed decisions for subsequent experiments. 

Ultimately, all configuration is done using various .json files which contain key-value pairs of the different configurable items. We further differentiate between different groups of configurations, which means that hyperparameters influencing the market, such as maximum possible prices or storage costs, are being handled separate from parameters needed for Reinforcement-Learning Agents, such as their learning rates, allowing users to make informed decisions when tweaking parameters involving different parts of the framework.

Our main goals when building our configuration tools were to make sure that users are able to quickly and safely configure their experiments in a way that is also easily reproducible. To be as user-friendly as possible, a lot of validation logic has been implemented to be able to catch invalid configurations as early as possible, making sure that whenever a training session is started, it is confirmed that the configuration is valid an the training will not fail due to wrongly set parameters at any point. Since our project has also been deployed to a remotely accessible, high-performant machine, we decided on creating an approachable web-interface for our configuration tool, which can now be used for both configuring, starting and evaluating experiments.

\subsection{The webserver/Docker-API}

\todo{Example screenshot of the webserver. Configuration and running page?}
On our webserver, users can upload .json files containing any number of (valid) key-value pairs, or create configurations using a form that always makes sure that it contains only valid values. For example, if the user chooses a circular economy market scenario with rebuy prices enabled, the user will not be able to configure an agent that cannot operate on such a market.

Aside from uploading or configuring complete, ready-to-go configurations, users can also choose to upload or create incomplete configurations, which can then be combined with other configuration objects to create a complete, valid configuration. This allows users to create multiple incomplete configurations containing only select parameters and then test all permutations of these configurations to observe the effect the different parameter combinations have on the experiment. 

An example of one such approach can be found below:
\todo{Example of a mix and match of parameters}

The webserver then also offers the option of starting multiple runs of the same configuration simultaneously. Most of the times this is recommended, as singular runs are prone to misleading results due to the trained agents training with specific market states. By running the same configuration multiple times using different starting circumstances for both the agent that is to be trained as well as its competitors, a configuration's effect on the agent's performance can be disconnected from the pseudo-random market states which influence the vendor's decision making processes.

\todo{Is this perhaps too much interpretation for this section?} A configuration can be interpreted as producing "reliable" agents if any number of simulations all converge on similar agent performances (meaning similar mean rewards). The more singular runs diverge from the mean \todo{mean or median? What do I use?}, the more the agent's performance is dependent on the initial market state, meaning that its performance is less stable, as it will produce unpredictable results on new, unknown market scenarios. A higher number of runs that produce agents which perform on similar levels of performance means that the specific configuration, to be axact the parameters responsible for the Reinforcement-Learning algorithm, produces "reliable" agents: No matter the state of the market, the agent can always be expected to perform on the same level that was observed during the training process, setting prices that lead to similar profits. Reliability is therefore one of the most important factors in determing an agent's overall quality, as the ultimate goal is always to be able to deploy the agent onto the real market, which will always differ from the simulated environments the agent experienced during training, simply due to the fact that real markets are being influenced by so many more parameters than one could model in our, or for that matter any simulation framework. 

\section{Choosing what to show the user during training}

During the training process, we of course record all actions the different vendors take, both for the training agent and its competitors, be they also Reinforcement-Learning Agents or Rulebased agents. Market states and events are also being recorded, to be able to match them to the agent's actions later on. Users of course want to have an indicator of how the training run is going so far, both to inform themselves about the total progress of the training, but also about the quality of the agent they are training. We now have to decide what information is shown to the user, making sure that the user is well informed, whilst also holding back on displaying too much information at once, as the underlying data and states are ever-evolving, leading to many datapoints being invalidated shortly after their creation.

In \nameref{section:WhenToMonitorWhat} we introduced the idea of running monitoring sessions while a training session is still runnning. To enable this, we need to save 

In the following section, we will present a number of datapoints that could be chosen to be shown to the user, together with an analysis on where their strengths in informing the user lie.

\subsection{Informing the user}



\section{Saving models at certain stages during training}
\subsection{Live-monitoring tools (see Chapter 4)}

\section{After training/Complete agents}
\subsection{Why do we even monitor these agents?}
\subsection{Which datapoints prove to be/are most effective?}