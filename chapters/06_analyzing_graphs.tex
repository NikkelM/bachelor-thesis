\label{ch:AnalyzingGraphs}

\begin{jointwork}
	In this chapter, we will put the tools and workflows described in the previous sections to the use. We will train a Reinforcement-Learning Agent, and then monitor it using all of the tools at our disposal, highlighting where each tool is most useful, and what could be improved.
\end{jointwork}

\section*{Setting up the experiment}

% Before starting our monitoring, we will be conducting two different experiments, each with a different Reinforcement-Learning algorithm and a different market environment. Both experiments will however be conducted on the same marketplace type, a Circular Economy model with rebuy-prices. To ensure that we are evaluating an agent with a performance that is to be expected with the provided parameters after the experiments have finished, we will run both the experiments multiple times to be able to identify outliers in the data. Diagrams of different versions of an experiment will be denoted using an underscore for differentiation.

% \subsection*{SAC-Duopoly}

% For the first experiment, we will train a Reinforcement-Learning Agent using the SAC-algorithm~\cite{StableBaselines3SAC} on a Duopoly marketplace with rebuy prices. The agent will be trained against a Rule-Based agent, more specifically the \emph{RuleBasedCERebuyAgentCompetitive}, as presented in \nameref{subsec:DataDrivenModels}. The configuration files for this experiment can be found in \Cref{fig:SACDuopolyConfigEnvironment}, \Cref{fig:SACDuopolyConfigMarket} and \Cref{fig:SACDuopolyConfigAgent}. We will refer to this experiment as the \emph{SAC-Duopoly}.

%% Text for only one experiment
Before starting our monitoring, we will need to conduct an experiment, where a Reinforcement-Learning algorithm is being trained on a market environment. For our experiment, we will train a Reinforcement-Learning agent using the SAC-algorithm~\cite{StableBaselines3SAC} on a Duopoly marketplace with rebuy prices. The agent will be trained playing against a Rule-Based agent, more specifically the \emph{RuleBasedCERebuyAgentCompetitive}, as presented in \fullref{subsec:DataDrivenModels}. The configuration files for this experiment can be found in \Cref{fig:SACDuopolyConfigEnvironment}, \Cref{fig:SACDuopolyConfigMarket} and \Cref{fig:SACDuopolyConfigAgent}. We will refer to this experiment as the \emph{SAC-Duopoly} experiment. To ensure that we are evaluating an agent with a performance that is to be expected with the provided parameters, we will conduct the experiment multiple times to be able to identify outliers in the data. All diagrams except those shown in \Cref{fig:SACDuopolyProfitsMean} (which contains diagrams from each of the four experiment runs) have been taken from the same experiment run, denoted as SAC-Duopoly\_1 in \Cref{fig:SACDuopolyProfitsMean1}.

% \subsection*{PPO-Oligopoly}

% For the second experiment, we will be training a different Reinforcement-Learning agent on a more complex market with more participants: A PPO-agent~\cite{StableBaselines3PPO} will be trained on an Oligopoly-Scenario, again with rebuy prices enabled. Three different Rule-Based agents will compete against the PPO-Agent on the same market:

% \begin{enumerate}
% 	\item A \emph{FixedPriceAgent}, which will always set the following three prices:
% 	      \begin{itemize}
% 		      \item New price: 7
% 		      \item Refurbished price: 4
% 		      \item Rebuy price: 2
% 	      \end{itemize}
% 	      It should be noted that the configured maximum possible price is 10, which can also be configured using the configuration files. Depending on this maximum price, the prices set by \emph{FixedPriceAgents} must be adjusted to allow them to realistically compete in the market.
% 	\item The second agent is a \emph{RuleBasedCERebuyAgentCompetitive}, a more complicated and sophisticated version of the simple \emph{RuleBasedCERebuyAgent}, which was introduced in \nameref{subsec:InventoryBasedModels}. The competitive version used for this experiment combines features of both the basic \emph{RuleBasedCERebuyAgent} and the \emph{RuleBasedCERebuyAgentStorageMinimizer}. It's policy implementation can be found in \todo{Insert code snippet in Appendix}xyz.\todo{Check if the agents are correct/order is correct}
% 	\item For the third competitor, we will again be using the \emph{RuleBasedCERebuyAgentStorageMinimizer}.
% \end{enumerate}
% This experiment will be referred to as the \emph{PPO-Oligopoly}.\todo{Config files for experiment, reuse same market config figure.}

\section*{Experiment results}

In the following sections we will use our different monitoring tools on the results of the SAC-Duopoly experiment. We will start with the Live-monitoring tool, which runs automatically after the training run has completed and creates over 90 graphs and diagrams already. Due to this high number of available diagrams, we will always only look at a curated selection, highlighting those which give the best insights into the trained agent. We have also dedicated a section to diagrams which are being created, but not as useful to the user, where we will try to identify the reason for the lack of information that can be gained from these diagrams (\nameref{sec:UselessDiagrams}).

\subsection*{Live- and Agent-monitoring}\label{subsec:LiveMonitoringResults}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean1.pdf}\\
		\subcaption{SAC-Duopoly\_1, 2000 training episodes}\label{fig:SACDuopolyProfitsMean1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean2.pdf}\\
		\subcaption{SAC-Duopoly\_2, 2000 training episodes}\label{fig:SACDuopolyProfitsMean2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean3.pdf}\\
		\subcaption{SAC-Duopoly\_3, 2000 training episodes}\label{fig:SACDuopolyProfitsMean3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean4.pdf}\\
		\subcaption{SAC-Duopoly\_4, 6000 training episodes}\label{fig:SACDuopolyProfitsMean4}
	\end{subfigure}
	\caption{Profit per episode of four different training runs of an SAC-Agent on a Duopoly market.}\label{fig:SACDuopolyProfitsMean}
\end{figure}

This section will focus on the diagrams created by the Live-monitoring tool after training, which always runs an Agent-monitoring session as well, to immediately provide the user with many additional useful diagrams without the need to run the tool manually.

A commonly asked question when deciding on the quality of a Reinforcement-Learning agent is their \emph{stability}. If an algorithm is stable, the trained agent will produce similar results over multiple training sessions, on the condition that the parameters do not differ. Not only the rewards achieved at the end of the training will be very similar, but also the amount of episodes needed to reach certain thresholds. In the case of the SAC-Duopoly experiment, we ran the same configuration four times: The first three experiments were run using the exact same parameters, for the fourth experiment the amount of training episodes were tripled, meaning that the SAC-Agent had more time to alter its policy. \Cref{fig:SACDuopolyProfitsMean} shows the results of these four training sessions, created using the \nameref{subsec:LiveMonitoring} tool and visualizes the data collected during the training process. Although many other graphs are created (\Cref{tab:AllMetrics}), the visualization of the total profit of the agent is the most convenient to use when evaluating an agent's performance, as the total profit is the parameter which the agent is trying to optimize.

\Cref{fig:SACDuopolyProfitsMean} shows the stability of the SAC-Agent very well. The agent not only reached the break-even threshold of a reward of 0 after around 150 episodes in each of the four training runs, but no matter the total length of training (see the model in \Cref{fig:SACDuopolyProfitsDensity4}, which was trained three times as long as the others), the profit would always stabilize and stay at around 670. Had the monitoring tools shown that the agent performs worse than expected in some of the experiments, we could conclude that this particular algorithm is not fit for the type of work required by our market simulation.

We can also observe that the profits of the SAC-Agent and the Rule-Based agent (in the case of this experiment, a \emph{RuleBasedCERebuyAgentStorageMinimizer}) seem to be closely linked to each other in this particular scenario. In the beginning of each experiment, when the Reinforcement-Learning agent still knows very little about the market and makes great losses, the Rule-Based agent also has a hard time to perform well. However, as soon as the SAC-Agent starts to perform better, the Rule-Based agent is also able to increase its mean profits at around the same rate as the SAC-Agent. Even more interestingly, the agents not only increase their profits at the same rate, but also very quickly arrive at a point where they earn the same mean amount as the other.

This might lead one to believe that the two competitors' policies align closely with each other. To validate this claim, another type of diagram created by the Live-monitoring tool can be used: The densityplots. These diagrams visualize probability densities for the various datasets recorded. While the diagrams shown in \Cref{fig:SACDuopolyProfitsMean} simply visualize data that was collected during the training run, densityplots are created by running the Agent-monitoring tool, where the marketplace is simulated an additional time. This allows us to use the `intermediate' models we saved during the training run (see \nameref{subsec:LiveMonitoring}) to compare the Reinforcement-Learning Agent's policies at different points in time during the training.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity1.pdf}\\
		\subcaption{Model trained for 500 episodes}\label{fig:SACDuopolyProfitsDensity1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity2.pdf}\\
		\subcaption{Model trained for 1000 episodes}\label{fig:SACDuopolyProfitsDensity2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity3.pdf}\\
		\subcaption{Model trained for 1500 episodes}\label{fig:SACDuopolyProfitsDensity3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity4.pdf}\\
		\subcaption{Model trained for 2000 episodes}\label{fig:SACDuopolyProfitsDensity4}
	\end{subfigure}
	\caption{Probability densities for achieving a certain profit for four different training stages of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyProfitsDensity}
\end{figure}

\Cref{fig:SACDuopolyProfitsDensity} shows the densityplots of profit-per-episode for four different training stages of the SAC-Duopoly\_1 experiment. From this it can be concluded that the claim that the two competitors' policies align closely is incorrect: even though the mean reward within an episode is always very similar (\Cref{fig:SACDuopolyProfitsMean1}), the SAC-Agent achieves rewards which lie closer together, while the Rule-Based agent's rewards have more of a spread.

We can also observe that the models which were trained for longer are not necessarily better or even the same quality of the models with less experience. There is a significant improvement going from the model shown in \Cref{fig:SACDuopolyProfitsDensity1} to the one in \Cref{fig:SACDuopolyProfitsDensity2}, this shift in the probability density curve can be correlated with the maximum mean rewards the two models could achieve during the training: For the model trained for 500 episodes, this was around 450, for the other it was about 660 (\Cref{fig:SACDuopolyProfitsMean1}). Both of these values have respectively high probabilities of being reached during the second simulation after the training has concluded, i case of the model trained for 1000 episodes, this even coincides with the maximum in the density plot (\Cref{fig:SACDuopolyProfitsDensity2}). Going from the model trained for 1000 episodes to the next one, which was trained for 1500 episodes (\Cref{fig:SACDuopolyProfitsDensity3}), both the probability densities as well as the mean rewards stay very close to each other. From this we can conclude that training the SAC-Agent for more than 1000 episodes is very likely to not have a great effect on the maximum reward achievable by the algorithm. Going from the model trained for 1500 episodes to the last one, saved at the end of the training \Cref{fig:SACDuopolyProfitsDensity4}, we can however see a deterioration in performance: While the mean rewards hardly changed (\Cref{fig:SACDuopolyProfitsMean1}), the probability density curve got wider at its base, extending out further towards a reward of 400, and lowering the probability of achieving a reward of 700 from previously above 0.5\% to under 0.4\%. This means that the model which was trained for the longest time produces less predictable results than those trained less, which is a tame version of \emph{Catastrophic Forgetting}, as touched upon in \nameref{subsec:FutureLiveMonitoring}. The insights gained by our monitoring tools combined with the fact that we save `intermediate' models of the Reinforcement-Learning agent during training allows us to find the optimal trained model to use for further investigation and eventual deployment on the real market. In the case of the SAC-Duopoly\_1 experiment, the optimal model would be the one trained for 1000 episodes.

\subsubsection*{Further investigation}

Evaluating a model based on just the mean profits achieved by the monitored agents may not be enough for many users, which is why our Live-monitoring offers many other useful diagrams as well. In this section, we will take a look at a small selection of them. The graphs used will be from the SAC-Duopoly\_1 experiment.

Users may ask themselves how the profits achieved by the different vendors are split between the two available retail channels of new and refurbished products, how many products were bought back from customers or how much the vendors had to pay for storage of these used products. For all of these questions, the Live-monitoring (together with the Agent-monitoring) tool offers two types of diagrams: First, simple lineplots as shown in \Cref{fig:SACDuopolyProfitsMean} can be used, as well as scatterplots which visualize the exact data recorded during the episode, instead of the trends shown by the lineplots. \Cref{fig:SACDuopolyMixedGraphs} shows a number of different metrics, all of which are available as both linepots and scatterplots (\Cref{tab:AllMetrics}).

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs1.pdf}\\
		\subcaption{Storage costs for bought back products}\label{fig:SACDuopolyMixedGraphs1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs2.pdf}\\
		\subcaption{Rebuy-prices set by the vendors}\label{fig:SACDuopolyMixedGraphs2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs3.pdf}\\
		\subcaption{Customers that bought nothing}\label{fig:SACDuopolyMixedGraphs3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs4.pdf}\\
		\subcaption{New prices set by the vendors}\label{fig:SACDuopolyMixedGraphs4}
	\end{subfigure}
	\caption{Diagrams visualizing various datapoints collected during training of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyMixedGraphs}
\end{figure}

Many connections can be made when evaluating different diagrams side-by-side, such as the observation of the initially high storage costs of both vendors in \Cref{fig:SACDuopolyMixedGraphs1} being caused by high rebuy prices set by the agents (\Cref{fig:SACDuopolyMixedGraphs2}). High rebuy prices make it likely that customers are willing to sell back their products, which leads to (over)full inventories and high storage costs. High storage and rebuy costs will have then caused a policy change to set lower rebuy prices, thereby de-incentivizing customers to sell back as many products as they used to and lowering the agent's storage costs, increasing the profit margin.

The next pair of diagrams shows the connection between a high number of customer that buy nothing, as visualized in \Cref{fig:SACDuopolyMixedGraphs3} being the direct cause of high prices for new products, shown in \Cref{fig:SACDuopolyMixedGraphs4}. As mentioned in \nameref{sec:Customers}, the customers implemented in our framework make their purchase decisions based on the prices they are presented with, which leads to many customers choosing to buy nothing before paying prices as high as set by the vendors.
% The next pair of diagrams tells us two things: First, the vendors' initially low profits from selling new products as shown in \Cref{fig:SACDuopolyMixedGraphs3}, or in the bigger picture, low profits overall (\Cref{fig:SACDuopolyProfitsMean1}) were not caused by the vendors setting prices that are too low, but rather too high. This is confirmed by an initially very high number of customers which chose to not buy any of the products offered by the vendors (\Cref{fig:SACDuopolyMixedGraphs4}). Additionally, when comparing the total profits of the two vendors with the profits gained from selling only new products (\Cref{fig:SACDuopolyProfitsMean1} and \Cref{fig:SACDuopolyMixedGraphs3}), we can come to the conclusion that the SAC-Agent must have learned to prioritize selling refurbished products over new ones and keeping storage costs low, since we know that overall profits for the two vendors are about the same in the later parts of the training, but the SAC-Agent sells a lot less new products than its Rule-Based counterpart.

If the Agent-monitoring is run after a training session (with a number of intermediate models), it also creates violinplots for all of the collected metrics (\Cref{tab:AllMetrics}). A selection of such diagrams can be found in \Cref{fig:SACDuopolyViolinPlots}, showing the respective total profits (\Cref{fig:SACDuopolyViolinPlots1}) and storage costs (\Cref{fig:SACDuopolyViolinPlots2}) for each intermediate model of the trained SAC-Agent. As explained in \nameref{subsec:AgentMonitoring}, these plots visualize the probability distributions as shown in \Cref{fig:SACDuopolyProfitsDensity} in a more condensed way, providing additional data such as actual maximum, minimum and median values as well. The biggest upside of the Violinplots is however that they are able to show the distributions for all intermediate models in one diagram, which allows for even better comparisons. Looking at \Cref{fig:SACDuopolyViolinPlots1} we can immediately see the difference in the spread of total profits achieved between the models trained for 1500 and 2000 episodes respectively, for which we previously needed to consult and compare two different diagrams (\Cref{fig:SACDuopolyProfitsDensity3} and \Cref{fig:SACDuopolyProfitsDensity4}). Similarly, \Cref{fig:SACDuopolyViolinPlots2} is able to tell us something else we did not know before: the longer a model was trained for, the more likely it is that it will induce high storage costs, a trend which was not necessarily visible in \Cref{fig:SACDuopolyMixedGraphs1}.

Violinplots do however not replace the need for densityplots, as both have an equally useful way of displaying data. While the violinplots show rough distributions plotting against the real numbers on the y-axis, the densityplots show the concrete probability values for each possible datapoint.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsAllViolin.pdf}\\
		\subcaption{Total profits for all intermediate models}\label{fig:SACDuopolyViolinPlots1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyStorageCostsViolin.pdf}\\
		\subcaption{Storage costs for all intermediate models}\label{fig:SACDuopolyViolinPlots2}
	\end{subfigure}
	\caption{Violinplots showing a selection of collected data when running Agent-monitoring after training.}\label{fig:SACDuopolyViolinPlots}
\end{figure}

\subsection*{Exampleprinter}

All of the diagrams shown in the previous section were created as part of the automatic monitoring done after a training session has concluded. There are however two other tools at our disposal to monitor the trained Reinforcement-Learning agent after they have been trained. For this, we only need the intermediate models saved during and at the end of training. In the case of the SAC-Duopoly\_1 experiment, we will be using the model saved after 1000 episodes, as we discovered it had the best performance at the end of \nameref{subsec:LiveMonitoringResults}. The first tool we will now utilize is the Exampleprinter, as introduced in \fullref{subsec:Exampleprinter}.

To configure this monitoring run, we will again need three configuration files: one defining the task to be done (for which we will re-use \Cref{fig:SACDuopolyConfigEnvironment}, only exchanging the `task' keyword), and one for both the hyperparameter-configuration of the market and the SAC-Agent, for which we will also re-use the configuration files used for training (\Cref{fig:SACDuopolyConfigMarket} and \Cref{fig:SACDuopolyConfigAgent}). By using the same configuration files again, we can emulate the market the agent was initially trained on as closely as possible.

At the end of the monitoring session, we receive an animated overview diagram that cycles through all timesteps, allowing us to identify and examine potentially interesting timesteps in the simulation. Due to the nature of this diagram being animated, we will not be able to thoroughly examine the results of this monitoring tool in this thesis. Instead \Cref{fig:SACDuopolyExampleprinter17}, which shows the $17^{th}$ timestep of the simulation, can be used to gain an understanding of the information that can be gained from this tool.

\begin{figure}[t]
	\centering
	\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/exampleprinter/ExampleprinterStep17.pdf}\\
	\caption{Actions and market states during step 17 of the Exampleprinter session.}\label{fig:SACDuopolyExampleprinter17}
\end{figure}

\subsection*{Policyanalyzer}

Run two different Policyanalyzer-runs: Once with only one feature, once with two features. Show how the info can be used to inform on the agent's quality. Maybe show one instance where it is not very useful.

\subsection*{Agent-monitoring}

Even though most of the diagrams that are created through the Agent-monitoring tool are created if it is run immediately after a training session, it is disconnected from the Live-monitoring tool. There are two major reasons for this:

\subsubsection*{Testing Rule-Based pricing methods}

First, aside from training Reinforcement-Learning agents, even though this is the main focus, our market simulation framework can just as well be used to implement and test classically Rule-Based pricing methods, as we have done ourselve for our Rule-Based vendors. In order to make this workflow possible, users need to be able to use the Agent-monitoring tool, as it is the only way of running large-scale simulations of different marketplaces aside from training a Reinforcement-Learning agent, which is not an option for this usecase. \Cref{fig:RulebasedAgentMonitoring} shows the results of an Agent-monitoring session with a \emph{RuleBasedCERebuyAgent} playing against a \emph{RuleBasedCERebuyAgentStorageMinimizer}, a \emph{Data-driven model} that tries to keep the amount of products in its storage as low as possible. Its policy can be found in \Cref{fig:PolicyRuleBasedStorageMinmizer}. As was already mentioned in \nameref{sec:ExplainVendors}, we can see that the \emph{RuleBasedCERebuyAgent} performed significantly worse than the other Rule-Based agent (\Cref{fig:RulebasedAgentMonitoring1}). This is mainly due to the fact that it is a purely \emph{Inventory-based model}, while the \emph{RuleBasedCERebuyAgentStorageMinimizer} is a \emph{Data-driven} model. Certain characteristics of the \emph{RuleBasedCERebuyAgentStorageMinimizer} can also be seen in \Cref{fig:RulebasedAgentMonitoring2} and \Cref{fig:RulebasedAgentMonitoring3}, as it sells significantly less refurbished products, but is still able to make about the same amount of profits as the simpler \emph{RuleBasedCERebuyAgent}. This can be attributed to its quality of keeping storage costs low by minimizing inventory size.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/rulebased/RuleBasedMonitoringProfits.pdf}\\
		\subcaption{Total profits per vendor}\label{fig:RulebasedAgentMonitoring1}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/rulebased/RuleBasedMonitoringPurchasesRefurbished.pdf}\\
		\subcaption{Refurbished products sold}\label{fig:RulebasedAgentMonitoring2}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/rulebased/RuleBasedMonitoringProfitsRefurbished.pdf}\\
		\subcaption{Profits by refurbished sales}\label{fig:RulebasedAgentMonitoring3}
	\end{subfigure}
	\caption{Diagrams created during an Agent-monitoring session comparing two Rule-Based pricing methods.}\label{fig:RulebasedAgentMonitoring}
\end{figure}

\subsubsection*{Other competitors for trained agents}

The second reason for having the Agent-monitoring as a stand-alone tool is to be able to monitor trained Reinforcement-Learning agents not only with the competitors they were trained against, but any combination of other agents, including other trained Reinforcement-Learning agents.\todo{Do it for SAC-Duopoly experiment}

\section*{Less useful diagrams}\label{sec:UselessDiagrams}

This section is meant to highlight a number of diagrams which are not very useful, such as the `Statistics diagrams'. The reason for this can be twofold: Either the data shown in the diagram is not inertly useful, or the graph provides duplicated or confusing information.