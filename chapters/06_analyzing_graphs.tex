\label{ch:AnalyzingGraphs}

\begin{jointwork}
	In this chapter, we will put the tools and workflows described in the previous sections to the use. We will train two different Reinforcement-Learning Agents on different marketplaces, and then monitor them using all of the tools at our disposal, highlighting where each tool is most useful, and what could be improved.
\end{jointwork}

\section*{Setting up the experiments}

Before starting our monitoring, we will be conducting two different experiments, each with a different Reinforcement-Learning algorithm and a different market environment. Both experiments will however be conducted on the same marketplace type, a Circular Economy model with rebuy-prices. Both experiments will be conducted multiple times using the same configurations\todo{Why?}, and diagrams of different versions will be denoted using an underscore for differentiation.

\subsection*{SAC-Duopoly}

For the first experiment, we will train a Reinforcement-Learning Agent using the SAC-algorithm~\cite{StableBaselines3SAC} on a Duopoly marketplace with rebuy prices. The agent will be trained against a Rule-Based agent, more specifically the \emph{RuleBasedCERebuyAgentStorageMinimizer}, as presented in \nameref{subsec:DataDrivenModels}. The concrete configurations can be found in \Cref{fig:SACDuopolyConfigEnvironment} and \Cref{fig:SACDuopolyConfigMarket}. We will refer to this experiment as the \emph{SAC-Duopoly}.

\subsection*{PPO-Oligopoly}

For the second experiment, we will be training a different Reinforcement-Learning agent on a more complex market with more participants: A PPO-agent~\cite{StableBaselines3PPO} will be trained on an Oligopoly-Scenario, again with rebuy prices enabled. Three different Rule-Based agents will compete against the PPO-Agent on the same market:

\begin{enumerate}
	\item A \emph{FixedPriceAgent}, which will always set the following three prices:
	      \begin{itemize}
		      \item New price: 7
		      \item Refurbished price: 4
		      \item Rebuy price: 2
	      \end{itemize}
	      It should be noted that the configured maximum possible price is 10, which can also be configured using the configuration files. Depending on this maximum price, the prices set by \emph{FixedPriceAgents} must be adjusted to allow them to realistically compete in the market.
	\item The second agent is a \emph{RuleBasedCERebuyAgentCompetitive}, a more complicated and sophisticated version of the simple \emph{RuleBasedCEAgent}, which was introduced in \nameref{subsec:InventoryBasedModels}. The competitive version used for this experiment combines features of both the basic \emph{RuleBasedCEAgent} and the \emph{RuleBasedCERebuyAgentStorageMinimizer}. It's policy implementation can be found in \todo{Insert code snippet in Appendix}xyz.
	\item For the third competitor, we will again be using the \emph{RuleBasedCERebuyAgentStorageMinimizer}.
\end{enumerate}
This experiment will be referred to as the \emph{PPO-Oligopoly}.

\section*{Experiment results}

In the following sections we will use our different monitoring tools on the two experiments we conducted. We will start with the Live monitoring tools, which runs automatically after the training run and creates over 90 graphs and diagrams already. Due to this high number of available diagrams, we will always only look at a curated selection, highlighting those with the most and least use for the user.

\subsection*{Live monitoring}

\begin{figure}[!hbt]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean1.pdf}\\
		\subcaption{SAC-Duopoly\_1, 2000 training episodes}\label{fig:SACDuopolyProfitsMean1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean2.pdf}\\
		\subcaption{SAC-Duopoly\_2, 2000 training episodes}\label{fig:SACDuopolyProfitsMean2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean3.pdf}\\
		\subcaption{SAC-Duopoly\_3, 2000 training episodes}\label{fig:SACDuopolyProfitsMean3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean4.pdf}\\
		\subcaption{SAC-Duopoly\_4, 6000 training episodes}\label{fig:SACDuopolyProfitsMean4}
	\end{subfigure}
	\caption{Profit per episode of four different training runs of an SAC-Agent on a Duopoly market.}\label{fig:SACDuopolyProfitsMean}
\end{figure}

A commonly asked question when deciding on the quality of a Reinforcement-Learning Agent is their \emph{stability}\todo{cite?}. If an algorithm is stable, the trained agent will produce similar results over multiple training sessions, on the condition that the parameters do not differ. Not only the rewards achieved at the end of the training will be very similar, but also the amount of episodes needed to reach certain thresholds. In the case of the SAC-Duopoly experiment, we ran the same configuration four times: The first three experiments were run using the exact same parameters, for the fourth experiment the amount of training episodes were tripled, meaning that the SAC-Agent had more time to alter its policy. \Cref{fig:SACDuopolyProfitsMean} shows the results of these four training sessions, created using the \nameref{subsec:LiveMonitoring} tool and visualizes the data collected during the training process. Although many other graphs are created\todo{Table/List of all graphs needed}, the visualization of the total profit of the agent is the most convenient to use when evaluating an agent's performance, as the total profit is the parameter which the agent is trying to optimize.

\Cref{fig:SACDuopolyProfitsMean} shows the stability of the SAC-Agent very well. The agent not only reached the break-even threshold of a reward of 0 after around 150 episodes in each of the four training runs, but no matter the total length of training (see the model in \Cref{fig:SACDuopolyProfitsDensity4}, which was trained three times as long as the others), the profit would always stabilize and stay at around 670. Had the monitoring tools shown that the agent performs worse than expected in some of the experiments, we could conclude that this particular algorithm is not fit for the type of work required by our market simulation.

We can also observe that the profits of the SAC-Agent and the Rule-Based Agent (in the case of this experiment, a \emph{RuleBasedCERebuyAgentStorageMinimizer}) seem to be closely linked to each other in this particular scenario. In the beginning of each experiment, when the Reinforcement-Learning agent still knows very little about the market and makes great losses, the Rule-Based agent also has a hard time to perform well. However, as soon as the SAC-Agent starts to perform better, the Rule-Based agent is also able to increase its mean profits at around the same rate as the SAC-Agent. Even more interestingly, the agents not only increase their profits at the same rate, but also very quickly arrive at a point where they earn the same mean amount as the other.

This might lead one to believe that the two competitors' policies align closely with each other. To validate this claim, another type of diagram created by the Live monitoring tool can be used: The density plots. These diagrams visualize probability densities for the various datasets recorded. While the diagrams shown in \Cref{fig:SACDuopolyProfitsMean} simply visualize data that was collected during the training run, density plots are created by simulating the marketplace an additional time.\todo{This is explanatory, perhaps move it to the approaches chapter} This allows us to use the `intermediate' models we saved during the training run (see \nameref{subsec:LiveMonitoring}) to compare the Reinforcement-Learning Agent's policies at different points in time during the training.

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity1.pdf}\\
		\subcaption{Model trained for 500 episodes}\label{fig:SACDuopolyProfitsDensity1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity2.pdf}\\
		\subcaption{Model trained for 1000 episodes}\label{fig:SACDuopolyProfitsDensity2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity3.pdf}\\
		\subcaption{Model trained for 1500 episodes}\label{fig:SACDuopolyProfitsDensity3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity4.pdf}\\
		\subcaption{Model trained for 1999 episodes}\label{fig:SACDuopolyProfitsDensity4}
	\end{subfigure}
	\caption{Probability densities for achieving a certain profit for four different training stages of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyProfitsDensity}
\end{figure}

\Cref{fig:SACDuopolyProfitsDensity} shows the density plots of profit-per-episode for four different training stages of the SAC-Duopoly\_1 experiment. It can be concluded that the claim that the two competitors' policies align closely is incorrect. Even though the mean reward within an episode is always very similar (\Cref{fig:SACDuopolyProfitsMean}), the SAC-Agent achieves rewards which lie closer together, while the Rule-Based agent's rewards have more of a spread.

We can also observe that the SAC-Agent models which were trained for longer are not necessarily better or even the same quality of the earlier models. There is a significant improvement going from the model shown in \Cref{fig:SACDuopolyProfitsDensity1} to the one in \Cref{fig:SACDuopolyProfitsDensity2}, this shift in the probability density curve can be correlated with the maximum mean rewards the two models could achieve during the training: For the model trained on 500 episodes, this was around 480 (\Cref{fig:SACDuopolyProfitsMean1}), for the other it was about 650 (\Cref{fig:SACDuopolyProfitsMean2}). Both of these values have the respective highest probability of being achieved during the second simulation after the training has concluded, visualized as high points in the density plots. Going from the model trained on 1000 episodes to the next one, which trained on 1500 episodes (\Cref{fig:SACDuopolyProfitsDensity3}), both the probability densities as well as the mean rewards stay very close to each other. From this we can conclude that training the SAC-Agent for more than 1000 episodes is very likely to not have an effect on the maximum performance achievable by the algorithm. Going from the model trained on 1500 episodes to the last one, saved at the end of the training \Cref{fig:SACDuopolyProfitsDensity4}, we can however see a deterioration in performance: While the mean rewards hardly changed (\Cref{fig:SACDuopolyProfitsMean3} and \Cref{fig:SACDuopolyProfitsMean4}), the probability density curve got wider at its base, extending out further towards a reward of 400, and lowering the high point at a reward of about 700 from the previous 0.6\% to only 0.4\%. This means that the model which was trained for the longest time produces less predictable results than those trained less, which may at first seem counterintutitive, but is a common phenomenom when training Reinforcement-Learning agents, known as \emph{Catastrophic Forgetting} (see also \cite{CatastrophicForgetting}). The insights gained by our monitoring tools combined with the fact that we save `intermediate' models of the Reinforcement-Learning agent during training allows us to find the optimal trained model to use for further investigation and eventual deployment on the real market. In the case of the SAC-Duopoly\_1 experiment, the optimal model is the one trained for 1000 episodes.

\subsubsection*{Further investigation}

Evaluating a model based on just the mean profits achieved by the monitored agents may not be enough for many users, which is why our Live-monitoring offers many other useful diagrams as well. In this section, we will take a look at a small selection of them. The graphs used will be from the SAC-Duopoly\_1 experiment.

The question may arise how the profits achieved by the different vendors are split between the two available retail channels of new and refurbished products, how many products were bought back from customers or how much the vendors had to pay for storage of these used products. For all of these questions, the Live-monitoring offers two types of diagrams: First, the simple lineplots shown in \Cref{fig:SACDuopolyProfitsMean} can be used, as well as scatterplots which visualize the exact data recorded during the episode, instead of the trends which the lineplots show. \Cref{fig:SACDuopolyMixedGraphs} shows the different metrics mentioned above in the two different formats.

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs1.pdf}\\
		\subcaption{Amount of products sold back to vendors}\label{fig:SACDuopolyMixedGraphs1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs2.pdf}\\
		\subcaption{Storage costs for bought back products}\label{fig:SACDuopolyMixedGraphs2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs3.pdf}\\
		\subcaption{Profit from selling new products}\label{fig:SACDuopolyMixedGraphs3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs4.pdf}\\
		\subcaption{Number of customers that bought nothing}\label{fig:SACDuopolyMixedGraphs4}
	\end{subfigure}
	\caption{Diagrams visualizing various datapoints collected during training of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyMixedGraphs}
\end{figure}

\section*{Which graphs are how useful?}
\section*{What limitations are there?}
\section*{What can we do in the future to improve the graphs or the workflow?}
\section*{Can we see which hyperparameters influence the results in what ways using the diagrams?}
\section*{How can we improve the workflow (e.g. Grid-Search) with our analysis?}

