\label{ch:AnalyzingGraphs}

\begin{jointwork}
	In this chapter, we will put the tools and workflows described in the previous sections to the use. We will train a Reinforcement-Learning Agent, and then monitor it using all of the tools at our disposal, highlighting where each tool is most useful, and what could be improved.
\end{jointwork}

\section*{Setting up the experiment}

% Before starting our monitoring, we will be conducting two different experiments, each with a different Reinforcement-Learning algorithm and a different market environment. Both experiments will however be conducted on the same marketplace type, a Circular Economy model with rebuy-prices. To ensure that we are evaluating an agent with a performance that is to be expected with the provided parameters after the experiments have finished, we will run both the experiments multiple times to be able to identify outliers in the data. Diagrams of different versions of an experiment will be denoted using an underscore for differentiation.

% \subsection*{SAC-Duopoly}

% For the first experiment, we will train a Reinforcement-Learning Agent using the SAC-algorithm~\cite{StableBaselines3SAC} on a Duopoly marketplace with rebuy prices. The agent will be trained against a Rule-Based agent, more specifically the \emph{RuleBasedCERebuyAgentStorageMinimizer}, as presented in \nameref{subsec:DataDrivenModels}. The configuration files for this experiment can be found in \Cref{fig:SACDuopolyConfigEnvironment}, \Cref{fig:SACDuopolyConfigMarket} and \Cref{fig:SACDuopolyConfigAgent}. We will refer to this experiment as the \emph{SAC-Duopoly}.

%% Text from when there was only one experiment
Before starting our monitoring, we will need to conduct an experiment, where a Reinforcement-Learning algorithm is being trained on a market environment. For our experiment, we will train a Reinforcement-Learning agent using the SAC-algorithm~\cite{StableBaselines3SAC} on a Duopoly marketplace with rebuy prices. The agent will be trained playing against a Rule-Based agent, more specifically the \emph{RuleBasedCERebuyAgentStorageMinimizer}, as presented in \nameref{subsec:DataDrivenModels}. The configuration files for this experiment can be found in \Cref{fig:SACDuopolyConfigEnvironment} and \Cref{fig:SACDuopolyConfigMarket}. We will refer to this experiment as the \emph{SAC-Duopoly} experiment. To ensure that we are evaluating an agent with a performance that is to be expected with the provided parameters, we will conduct the experiment multiple times to be able to identify outliers in the data. All diagrams except those shown in \Cref{fig:SACDuopolyProfitsMean} (which contains diagrams from each of the four experiment runs) have been taken from the same experiment run, denoted as SAC-Duopoly\_1 in \Cref{fig:SACDuopolyProfitsMean1}.

% \subsection*{PPO-Oligopoly}

% For the second experiment, we will be training a different Reinforcement-Learning agent on a more complex market with more participants: A PPO-agent~\cite{StableBaselines3PPO} will be trained on an Oligopoly-Scenario, again with rebuy prices enabled. Three different Rule-Based agents will compete against the PPO-Agent on the same market:

% \begin{enumerate}
% 	\item A \emph{FixedPriceAgent}, which will always set the following three prices:
% 	      \begin{itemize}
% 		      \item New price: 7
% 		      \item Refurbished price: 4
% 		      \item Rebuy price: 2
% 	      \end{itemize}
% 	      It should be noted that the configured maximum possible price is 10, which can also be configured using the configuration files. Depending on this maximum price, the prices set by \emph{FixedPriceAgents} must be adjusted to allow them to realistically compete in the market.
% 	\item The second agent is a \emph{RuleBasedCERebuyAgentCompetitive}, a more complicated and sophisticated version of the simple \emph{RuleBasedCEAgent}, which was introduced in \nameref{subsec:InventoryBasedModels}. The competitive version used for this experiment combines features of both the basic \emph{RuleBasedCEAgent} and the \emph{RuleBasedCERebuyAgentStorageMinimizer}. It's policy implementation can be found in \todo{Insert code snippet in Appendix}xyz.
% 	\item For the third competitor, we will again be using the \emph{RuleBasedCERebuyAgentStorageMinimizer}.
% \end{enumerate}
% This experiment will be referred to as the \emph{PPO-Oligopoly}.\todo{Config files for experiment, reuse same market config figure.}

\section*{Experiment results}

In the following sections we will use our different monitoring tools on the results of the SAC-Duopoly experiment. We will start with the Live-monitoring tool, which runs automatically after the training run has completed and creates over 90 graphs and diagrams already. Due to this high number of available diagrams, we will always only look at a curated selection, highlighting those which give the best insights into the trained agent. We have also dedicated a section to diagrams which are being created, but not as useful to the user, where we will try to identify the reason for the lack of information that can be gained from these diagrams (\nameref{sec:UselessDiagrams}).

\subsection*{Live- and Agent-monitoring}\label{subsec:LiveMonitoringResults}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean1.pdf}\\
		\subcaption{SAC-Duopoly\_1, 2000 training episodes}\label{fig:SACDuopolyProfitsMean1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean2.pdf}\\
		\subcaption{SAC-Duopoly\_2, 2000 training episodes}\label{fig:SACDuopolyProfitsMean2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean3.pdf}\\
		\subcaption{SAC-Duopoly\_3, 2000 training episodes}\label{fig:SACDuopolyProfitsMean3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsMean4.pdf}\\
		\subcaption{SAC-Duopoly\_4, 6000 training episodes}\label{fig:SACDuopolyProfitsMean4}
	\end{subfigure}
	\caption{Profit per episode of four different training runs of an SAC-Agent on a Duopoly market.}\label{fig:SACDuopolyProfitsMean}
\end{figure}

This section will focus on the diagrams created by the Live-monitoring tool after training, which always runs an Agent-monitoring session as well, to immediately provide the user with many additional useful diagrams without the need to run the tool manually.

A commonly asked question when deciding on the quality of a Reinforcement-Learning agent is their \emph{stability}. If an algorithm is stable, the trained agent will produce similar results over multiple training sessions, on the condition that the parameters do not differ. Not only the rewards achieved at the end of the training will be very similar, but also the amount of episodes needed to reach certain thresholds. In the case of the SAC-Duopoly experiment, we ran the same configuration four times: The first three experiments were run using the exact same parameters, for the fourth experiment the amount of training episodes were tripled, meaning that the SAC-Agent had more time to alter its policy. \Cref{fig:SACDuopolyProfitsMean} shows the results of these four training sessions, created using the \nameref{subsec:LiveMonitoring} tool and visualizes the data collected during the training process. Although many other graphs are created (\Cref{tab:AllMetrics}), the visualization of the total profit of the agent is the most convenient to use when evaluating an agent's performance, as the total profit is the parameter which the agent is trying to optimize.

\Cref{fig:SACDuopolyProfitsMean} shows the stability of the SAC-Agent very well. The agent not only reached the break-even threshold of a reward of 0 after around 150 episodes in each of the four training runs, but no matter the total length of training (see the model in \Cref{fig:SACDuopolyProfitsDensity4}, which was trained three times as long as the others), the profit would always stabilize and stay at around 670. Had the monitoring tools shown that the agent performs worse than expected in some of the experiments, we could conclude that this particular algorithm is not fit for the type of work required by our market simulation.

We can also observe that the profits of the SAC-Agent and the Rule-Based agent (in the case of this experiment, a \emph{RuleBasedCERebuyAgentStorageMinimizer}) seem to be closely linked to each other in this particular scenario. In the beginning of each experiment, when the Reinforcement-Learning agent still knows very little about the market and makes great losses, the Rule-Based agent also has a hard time to perform well. However, as soon as the SAC-Agent starts to perform better, the Rule-Based agent is also able to increase its mean profits at around the same rate as the SAC-Agent. Even more interestingly, the agents not only increase their profits at the same rate, but also very quickly arrive at a point where they earn the same mean amount as the other.

This might lead one to believe that the two competitors' policies align closely with each other. To validate this claim, another type of diagram created by the Live-monitoring tool can be used: The density plots. These diagrams visualize probability densities for the various datasets recorded. While the diagrams shown in \Cref{fig:SACDuopolyProfitsMean} simply visualize data that was collected during the training run, density plots are created by running the Agent-monitoring tool, where the marketplace is simulated an additional time.\todo{This is explanatory, perhaps move it to the approaches chapter} This allows us to use the `intermediate' models we saved during the training run (see \nameref{subsec:LiveMonitoring}) to compare the Reinforcement-Learning Agent's policies at different points in time during the training.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity1.pdf}\\
		\subcaption{Model trained for 500 episodes}\label{fig:SACDuopolyProfitsDensity1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity2.pdf}\\
		\subcaption{Model trained for 1000 episodes}\label{fig:SACDuopolyProfitsDensity2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity3.pdf}\\
		\subcaption{Model trained for 1500 episodes}\label{fig:SACDuopolyProfitsDensity3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsDensity4.pdf}\\
		\subcaption{Model trained for 2000 episodes}\label{fig:SACDuopolyProfitsDensity4}
	\end{subfigure}
	\caption{Probability densities for achieving a certain profit for four different training stages of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyProfitsDensity}
\end{figure}

\Cref{fig:SACDuopolyProfitsDensity} shows the density plots of profit-per-episode for four different training stages of the SAC-Duopoly\_1 experiment. From this it can be concluded that the claim that the two competitors' policies align closely is incorrect: even though the mean reward within an episode is always very similar (\Cref{fig:SACDuopolyProfitsMean1}), the SAC-Agent achieves rewards which lie closer together, while the Rule-Based agent's rewards have more of a spread.

We can also observe that the models which were trained for longer are not necessarily better or even the same quality of the models with less experience. There is a significant improvement going from the model shown in \Cref{fig:SACDuopolyProfitsDensity1} to the one in \Cref{fig:SACDuopolyProfitsDensity2}, this shift in the probability density curve can be correlated with the maximum mean rewards the two models could achieve during the training: For the model trained on 500 episodes, this was around 450, for the other it was about 660 (\Cref{fig:SACDuopolyProfitsMean1}). Both of these values have respectively high probabilities of being reached during the second simulation after the training has concluded, i case of the model trained for 1000 episodes, this even coincides with the maximum in the density plot (\Cref{fig:SACDuopolyProfitsDensity2}). Going from the model trained on 1000 episodes to the next one, which was trained on 1500 episodes (\Cref{fig:SACDuopolyProfitsDensity3}), both the probability densities as well as the mean rewards stay very close to each other. From this we can conclude that training the SAC-Agent for more than 1000 episodes is very likely to not have a great effect on the maximum reward achievable by the algorithm. Going from the model trained on 1500 episodes to the last one, saved at the end of the training \Cref{fig:SACDuopolyProfitsDensity4}, we can however see a deterioration in performance: While the mean rewards hardly changed (\Cref{fig:SACDuopolyProfitsMean1}), the probability density curve got wider at its base, extending out further towards a reward of 400, and lowering the probability of achieving a reward of 700 from previously above 0.5\% to under 0.4\%. This means that the model which was trained for the longest time produces less predictable results than those trained less, which is a tame version of \emph{Catastrophic Forgetting}, as touched upon in \nameref{subsec:FutureLiveMonitoring}. The insights gained by our monitoring tools combined with the fact that we save `intermediate' models of the Reinforcement-Learning agent during training allows us to find the optimal trained model to use for further investigation and eventual deployment on the real market. In the case of the SAC-Duopoly\_1 experiment, the optimal model would be the one trained for 1000 episodes.

\subsubsection*{Further investigation}

Evaluating a model based on just the mean profits achieved by the monitored agents may not be enough for many users, which is why our Live-monitoring offers many other useful diagrams as well. In this section, we will take a look at a small selection of them. The graphs used will be from the SAC-Duopoly\_1 experiment.

Users may ask themselves how the profits achieved by the different vendors are split between the two available retail channels of new and refurbished products, how many products were bought back from customers or how much the vendors had to pay for storage of these used products. For all of these questions, the Live-monitoring (together with the Agent-monitoring) tool offers two types of diagrams: First, simple lineplots as shown in \Cref{fig:SACDuopolyProfitsMean} can be used, as well as scatterplots which visualize the exact data recorded during the episode, instead of the trends shown by the lineplots. \Cref{fig:SACDuopolyMixedGraphs} shows a number of different metrics such as the ones mentioned above, all of which are available as both linepots and scatterplots (\Cref{tab:AllMetrics}).

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs1.pdf}\\
		\subcaption{Amount of products sold back to vendors}\label{fig:SACDuopolyMixedGraphs1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs2.pdf}\\
		\subcaption{Storage costs for bought back products}\label{fig:SACDuopolyMixedGraphs2}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs3.pdf}\\
		\subcaption{Profit from selling new products}\label{fig:SACDuopolyMixedGraphs3}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyMixedGraphs4.pdf}\\
		\subcaption{Customers that bought nothing}\label{fig:SACDuopolyMixedGraphs4}
	\end{subfigure}
	\caption{Diagrams visualizing various datapoints collected during training of the SAC-Duopoly\_1 experiment.}\label{fig:SACDuopolyMixedGraphs}
\end{figure}

Many connections can be made when evaluating different diagrams side-by-side, such as the observation of the initially high storage costs of the SAC-Agent in \Cref{fig:SACDuopolyMixedGraphs2} being caused by a very high number of products that were bought back from customers (\Cref{fig:SACDuopolyMixedGraphs1}), which is in turn a cause of high rebuy prices set by the Reinforcement-Learning Agent, making it more likely that customers sell back their products. High storage and rebuy costs will have then caused a policy change to set lower rebuy prices, thereby de-incentivizing customers to sell back as many products as they used to and lowering the agent's storage costs.

The next pair of diagrams tells us two things: First, the vendors' initially low profits from selling new products as shown in \Cref{fig:SACDuopolyMixedGraphs3}, or in the bigger picture, low profits overall (\Cref{fig:SACDuopolyProfitsMean1}) were not caused by the vendors setting prices that are too low, but rather too high. This is confirmed by an initially very high number of customers which chose to not buy any of the products offered by the vendors (\Cref{fig:SACDuopolyMixedGraphs4}). Additionally, when comparing the total profits of the two vendors with the profits gained from selling only new products (\Cref{fig:SACDuopolyProfitsMean1} and \Cref{fig:SACDuopolyMixedGraphs3}), we can come to the conclusion that the SAC-Agent must have learned to prioritize selling refurbished products over new ones and keeping storage costs low, since we know that overall profits for the two vendors are about the same in the later parts of the training, but the SAC-Agent sells a lot less new products than its Rule-Based counterpart.

If the Agent-monitoring is run after a training session (with a number of intermediate models), it also creates violinplots for all of the collected metrics (\Cref{tab:AllMetrics}). A selection of such diagrams can be found in \Cref{fig:SACDuopolyViolinPlots}, showing the respective total profits (\Cref{fig:SACDuopolyViolinPlots1}) and storage costs (\Cref{fig:SACDuopolyViolinPlots2}) for each intermediate model of the trained SAC-Agent. As explained in \nameref{subsec:AgentMonitoring}, these plots visualize the probability distributions as shown in \Cref{fig:SACDuopolyProfitsDensity} in a more condensed way, providing additional data such as actual maximum, minimum and median values as well. The biggest upside of the Violinplots is however that they are able to show the distributions for all intermediate models in one diagram, which allows for even better comparisons. Looking at \Cref{fig:SACDuopolyViolinPlots1} we can immediately see the difference in the spread of total profits achieved between the models trained for 1500 and 2000 episodes respectively, for which we previously needed to consult and compare two different diagrams (\Cref{fig:SACDuopolyProfitsDensity3} and \Cref{fig:SACDuopolyProfitsDensity4}). Similarly, \Cref{fig:SACDuopolyViolinPlots2} is able to tell us something else we did not know before: the longer a model was trained for, the more likely it is that it will induce high storage costs, a trend which was not necessarily visible in \Cref{fig:SACDuopolyMixedGraphs2}.

Violinplots do however not replace the need for density plots, as both have an equally useful way of displaying data. While the violinplots show rough distributions plotting against the real numbers on the y-axis, the density plots show the concrete probability values for each possible datapoint.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyProfitsAllViolin.pdf}\\
		\subcaption{Total profits for all intermediate models}\label{fig:SACDuopolyViolinPlots1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width = \textwidth]{images/experiments/SACDuopoly/SACDuopolyStorageCostsViolin.pdf}\\
		\subcaption{Storage costs for all intermediate models}\label{fig:SACDuopolyViolinPlots2}
	\end{subfigure}
	\caption{Violinplots showing a selection of collected data when running Agent-monitoring after training.}\label{fig:SACDuopolyViolinPlots}
\end{figure}

\subsection*{Exampleprinter}

Run an exampleprinter, insert one frame of svg. Mention that it is normally animated. Talk about why this offers more or new info than the Live/Agent-monitoring.

\subsection*{Policyanalyzer}

Run two different Policyanalyzer-runs: Once with only one feature, once with two features. Show how the info can be used to inform on the agent's quality. Maybe show one instance where it is not very useful.

\subsection*{Why not run Agent-monitoring? If not, why is it separate from Live-monitoring}

We do not run it, as it is already run with the Live-monitoring. So after training, we do not need to do it again. It still exists, as you may want to monitor the trained models independentl of each other using different parameters, or to monitor only Rule-Based agents.

\section*{Less useful diagrams}\label{sec:UselessDiagrams}

This section is meant to highlight a number of diagrams which are not very useful, such as the `Statistics diagrams'. The reason for this can be twofold: Either the data shown in the diagram is not inertly useful, or the graph provides duplicated or confusing information.

\section*{Some sort of conclusion here}

% \section*{Which graphs are how useful?}
% \section*{What limitations are there?}
% \section*{What can we do in the future to improve the graphs or the workflow?}