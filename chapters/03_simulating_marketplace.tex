\begin{jointwork}\label{ch:SimulatingMarketplace}
	This chapter will introduce the various components that make up a recommerce market, and how they were implemented in our simulation framework. We will take a brief look at different market scenarios as well as how our customers make decisions. The focus of this chapter will however lie on the vendors, the rule-based as well as the ones trained using Reinforcement learning algorithms. For further information on the overall framework structure refer to \cite{LeoThesis}, for detailed insights into our market processes, see \cite{NickThesis}.
\end{jointwork}

\section{Market scenarios}\label{sec:MarketScenarios}

In our framework, we implemented a number of `market blueprints' for classic Linear Economy markets as well as for Circular Economy markets both with and without rebuy prices. There are marketplaces available for each combination of the following two features:
\begin{enumerate}
	\item Marketplace type: Linear Economy, Circular Economy, Circular Economy with rebuy prices
	\item Market environment: Monopoly, Duopoly, Oligopoly
\end{enumerate}
The marketplace type defines the number of prices the vendor has to set. In a Linear Economy, vendors only set prices for new items, in a Circular Economy prices for refurbished items need to be set as well. Without rebuy prices, customers simply return products to vendors `for free'. In order to simulate a proper \emph{recommerce} market, users can choose a Circular Economy with rebuy prices.

The market environment defines the number of competing vendors in the simulation: Monopolistic and competitive markets are available, where the Duopoly is simply a particular version of an Oligopoly with only two competing vendors. Depending on the chosen market environment one, two, or any number of vendors can be chosen to be used in an experiment. \Cref{fig:OverviewDiagram} shows a reduced overview of the classes in our framework which concern themselves with the market simulation, and how they interact with each other during the simulation.

In the most common use case of our framework, the training of RL agents, only the agent (or multiple agents in the case of training multiple RL agents against each other) that is to be trained needs to be configured by the user, as each market environment is equipped with a pre-defined set of competitors that will play against the agent defined by the user. To allow for more control over the simulation, users are however also able to change these competitors to use any vendors they want - as long as they are a valid fit for the marketplace type and the number of competitors chosen matches the market environment. How to configure an experiment will be explained in \cbfref{ch:OurWorkflow}.

\begin{figure}[t]
	\centering
	\includegraphics[width = \textwidth]{images/overview_diagram.pdf}\\
	\caption{Interactions between classes concerning the market simulation.\\}\label{fig:OverviewDiagram}
\end{figure}

\section{Customers}\label{sec:Customers}

Customers are at the centre of every type of marketplace, which makes them an integral part of our market simulation. However, since each customer in the real market is an individual with different backgrounds and makes purchase decisions based on personal preferences, modelling a realistic depiction of real-world customers proves to be very difficult and time-consuming. For this reason, we decided to keep our initial implementation of the customers as simple as possible, taking into account future extension and scalability concerns.

Most customers' behaviour can be classified into one of a (non-exhaustive) number of categories, such as those proposed in~\cite{ShoppingStyles}, see \Cref{tab:shoppingStyles}. As we are focussing on dynamic pricing and our vendors can only change/influence the prices of their products, we decided on primarily building customers that value price over any other features a product may have, thereby incentivising our vendors to make the most of their pricing power. This behaviour coincides with the proposed shopping style of the \emph{Price Conscious} or \emph{Value for money} consumer.

To make Linear markets a little more complex and add another dimension than just the \emph{new price} of a product for vendors to consider, random quality values are assigned to each vendor's products at the start of an episode. Customers in this economy model were modelled to take this quality value into account when making their purchasing decisions, further reinforcing the shopping style mentioned above. As Circular Economy markets are inherently more complex than their linear counterparts (through the addition of two new price channels, which influence each other through their effect on customer behaviour), it was decided to remove the additional layer of quality values from these markets.

Within each simulated time step, after the various vendors have set their prices, purchasing decisions are made by the customers. To save on computational time, probability distributions are used to determine what part of the total number of customers will decide to take which action, instead of simulating each customer individually. See \Cref{def:customerDecisions} and \Cref{def:customerSoftmax} for the way these distributions are calculated for an exemplary recommerce marketplace, for further information also see \cite{NickThesis}.

\clearpage
\begin{definition}\label{def:customerDecisions}
	Let \(P_{i, new}\) be the price of the new and \(P_{i, ref}\) the price of the refurbished product set by vendor \(i\), with \(0 \leq i < n\) and \(n \in \mathbb{N}\) the number of vendors in the market. Prices will always be within the range \([0,10]\). We now define \(r_{new}(P)\) as the function determining the customers' \emph{preference ratio} regarding a new price \(P\) set by any vendor:
	\begin{equation}\label{eq:NewPref}
		r_{new}(P) := \frac{10}{P} - e^{P - 8}
	\end{equation}
	and similarly, \(r_{ref}(P)\) as the preference ratio regarding the refurbished price:
	\begin{equation}\label{eq:RefPref}
		r_{ref}(P) := \frac{5.5}{P} - e^{P - 5}.
	\end{equation}
	Additionally, there is a static preference \(r_{not}\) that any customer will opt to buy no product:
	\begin{equation}\label{eq:NothingPref}
		r_{not} = 1.
	\end{equation}
\end{definition}

Following the numerators of the two fractions, customers adjust their determined cost-benefit ratios to value a refurbished product to be 55\% the quality of that of a new product. This adjustment is necessary as there is no inherent quality value for products in our simulated recommerce markets. Additionally, the constants of 8 and 5 respectively in the exponent of the substracted exponential function act as price thresholds above which it becomes exponentially more unlikely that a customer will choose the respective product, as the value of the preference ratio will decrease.

\Cref{def:customerSoftmax} explains how we use the \emph{softmax} function to normalize our values, after which each preference ratio will be within the interval \((0,1)\), with all ratios adding up to \(1\), as is required to use them as a probability distribution. We use \emph{softmax}, as it is able to deal with the possibly negative results of the two \(r\)-functions.

\begin{definition}\label{def:customerSoftmax}
	We keep the definitions from \Cref{def:customerDecisions}. Now, let \(S\) be the sum of the exponentials of the preference ratios for all \(n\) vendors and the preference to buy nothing:
	\begin{equation}\label{eq:PrefSum}
		S := e^{r_{not}} + \sum_{i=0}^{n}\biggl(e^{r_{new}(P_{i, new})}+e^{r_{ref}(P_{i, ref})}\biggr).
	\end{equation}
	We can now normalize the preference ratios and determine the concrete probabilities of customers choosing to buy the new, refurbished or no product by using the \emph{softmax} function on our preference ratios. Let \(\pi_{i, j}\) be the probability that a customer chooses to buy product \(j\) from vendor \(i\), with \(0 \leq i < n\) and \(j \in \{new, ref\}\):
	\begin{equation}\label{eq:Softmax}
		\pi_{i, j} := \frac{e^{r_{j}(P_{i, j})}}{S}
	\end{equation}
	and let \(\pi_{not}\) be the probability of a customer buying no product:
	\begin{equation}
		\pi_{not} := \frac{e^{r_{n}}}{S}.
	\end{equation}
	All of these probabilities are collected in the set \(\Pi\). Following the definition of \emph{softmax}, all probabilities in \(\Pi\) sum up to 1, as required.
\end{definition}

Following this conversion of the preference ratios, the market uses a standard \emph{multinomial} distribution to draw \(m\) samples out of the pool of probabilities \(\Pi\), with \(m \in \mathbb{N}\) being the number of customers making a purchasing decision in this step of the simulation. The result of this sampling decides how many customers buy which product. See \bfref{subsec:customerPropExample} for an exemplary calculation of such distributions.

As mentioned at the start of this section, the current decisionmaking process of customers in our simulation is still quite basic. \bfref{sec:DivergingFromRealMarket} introduces a number of parameters and circumstances that can be used to make customer behaviour more realistic. Through the modular approach when building the framework, any customer behaviour implemented in the future can easily be added to the pool of available options, as long as it manifests in the form of a probability distribution, at which point the number of customers can also be split between different distributions when drawing from the \emph{multinomial} distribution (or any other distribution if so desired).

\subsection{Exemplary calculation of purchase probabilities}\label{subsec:customerPropExample}

\begin{wraptable}{r}{4.25cm}
	\begin{tabular}{|r|c|c|}
		\hline
		         & \(P_{new}\) & \(P_{ref}\) \\\hline
		Vendor 0 & 7           & 4           \\\hline
		Vendor 1 & 6           & 2           \\\hline
	\end{tabular}
	\caption{Prices set by the vendors.}\label{tab:CustomerExamplePrices}
\end{wraptable}

In this section, we will calculate the probability distribution for an exemplary market scenario. We assume a Circular Economy, Duopoly market scenario with prices set by the two vendors as shown in \Cref{tab:CustomerExamplePrices}. This will result in the following preference ratios for new products, following the definition in eq. \eqref{eq:NewPref}:

\begin{equation}
	r_{0,new} := r_{new}(P_{0,new}) = \frac{10}{7} - e^{7 - 8} \approx 1.06
\end{equation}
\begin{equation}
	r_{1,new} := r_{new}(P_{1,new}) = \frac{10}{6} - e^{6 - 8} \approx 1.53
\end{equation}

We can immediately see that customers prefer the new product offered by vendor 1 over the one offered by vendor 0, signified by the higher preference ration, which is due to the lower price of the product offered by vendor 1. The same calculation can now be done for the refurbished products following eq. \eqref{eq:RefPref}:

\begin{equation}
	r_{0,ref} := r_{ref}(P_{0,ref}) = \frac{5.5}{4} - e^{4 - 5} \approx 1.01
\end{equation}
\begin{equation}
	r_{1,ref} := r_{ref}(P_{1,ref}) = \frac{5.5}{2} - e^{2 - 5} \approx 2.70
\end{equation}

It seems that a price of 2 for a refurbished item is a great deal. In order to draw samples from the \emph{multinomial} distribution, we must now normalize our preference ratios using \emph{softmax}. For this, we first calculate the sum over all preference ratios as defined in eq. \eqref{eq:PrefSum} (including the `nothingpreference' of eq. \eqref{eq:NothingPref}):

\begin{equation}
	S = e^{1} + e^{1.06} + e^{1.53} + e^{1.01} + e^{2.70} \approx 27.85
\end{equation}

\begin{wraptable}[5]{r}{5.4cm}
	\begin{tabular}{|r|c|c|c|}
		\hline
		         & \(\pi_{new}\) & \(\pi_{ref}\) & \(\pi_{not}\)         \\\hline
		Vendor 0 & 0.10          & 0.10          & \multirow{2}{*}{0.10} \\\cline{1-3}
		Vendor 1 & 0.17          & 0.53          &                       \\\hline
	\end{tabular}
	\caption{Purchase probabilites.}\label{tab:CustomerExampleSoftmax}
\end{wraptable}

Using this sum, we can now calculate the purchase probabilities for each product using eq. \eqref{eq:Softmax}, the results of which can be seen in \Cref{tab:CustomerExampleSoftmax}. These probabilites sum to 1, as required, and can subsequently be used to draw samples from the multinomial distribution, which will be omitted here.

\subsection{Owners}

Once a customer has decided to buy a product from any of the available vendors, they turn into an \emph{owner}. In the next step of the simulation, they are offered the option of selling their now used product back to one of the vendors. If they decide to do so, the vendor pays them the advertised \emph{rebuy price} and adds the used product to their inventory, from where it can then be sold as a refurbished product in the following step.

In our simulation, all owners are memoryless, meaning that they do not remember the original price they paid for the product. Additionally, each vendor in the market is obligated to buy back any product, independent of the original vendor. In each step, every owner has the option to either keep the product, discard it (meaning it is removed from the market and not sold back to a vendor), or sell it to any one of the vendors in the market. Similar to the way we compute customer purchasing decisions, the decisions owners take are also represented through probability distributions. Within each time step, a constant percentage of all owners, currently defined to be 5\%, will either return or discard (throw away) their product, with the rest of the owners keeping their product for at least one more time step. When deciding what to do, owners act according to the following preference ratios:

\begin{definition}\label{def:ownerDecisions}
	We keep the variables defined in \Cref{def:customerDecisions}. Additionally, let \(P_{i,back}\) be the price at which vendor \(i\) is willing to buy back an owner's product (the \emph{rebuy price}). \(P_{i, back}\) will be within the range of \([0,10]\). For each vendor \(i\), we define \(P_{i,min}\) as the vendor's purchase option with the lowest price:
	\begin{equation}
		P_{i, min} := \min(P_{i, new}, P_{i, ref}).
	\end{equation}
	We define \(r_{back}(P)\) as the function determining an owner's \emph{preference ratio} to sell their product to vendor \(i\), \(0 \leq i < n\) regarding the rebuy price:
	\begin{equation}
		r_{back}(P) := 2 \cdot e^{\frac{P_{i, back} - P_{i, min}}{P_{i, min}}}.
	\end{equation}
	Additionally, the owner's \emph{discard preference} \(r_{disc}\) is updated for each vendor as follows:
	\begin{equation}
		r_{disc} := \min\biggl({r_{disc}, \frac{2}{P_{i, back} + 1}}\biggr).
	\end{equation}
	Meaning the owner is more likely to discard their product if rebuy prices are low across the board.
\end{definition}

Equal to the way our customers work, we again normalize these preference ratios using the \emph{softmax} function defined in \Cref{def:customerSoftmax} (substituting \(r_{not}\) with \(r_{disc}\) and the ratios for new and refurbished products with \(r_{back}\)) and subsequently draw samples from a \emph{multinomial} distribution.

\clearpage
\section{Vendors}\label{sec:ExplainVendors}

Vendors are the main focus of our market simulation. While our framework will not be able to reproduce all types of pricing models used in the real market, we strive to model as many different models as possible (and feasible in the scope of the project). Due to the modular nature of the framework, it is possible for users to easily create and add their own pricing strategies in the form of a vendor that can play on a market. This allows users to not only add other RL algorithms, but also to define new rule-based strategies. For this reason, we also do not restrict the usage of our monitoring tools to just RL agents, but allow users to monitor type of vendor, independent of the underlying way in which it computes prices.

We will use four types of dynamic pricing models, as defined in `Dynamic pricing models for electronic business'~\cite{dynamicPricingModels}, to describe the different models we implemented in our framework. These categories are not mutually exclusive, and many agents belong to more than one category.

\subsection{Inventory-based models}\label{subsec:InventoryBasedModels}

These are pricing models which are based on inventory levels and similar parameters, such as the number of items in circulation (items which are currently in use by customers). In our framework, almost all rule-based agents consider their inventory levels when deciding which prices to set. The only exception to this rule are the simplest of our agents, the \emph{FixedPriceAgents}, which will always set the same prices, no matter the current market state and competitor actions. The prices these agents will set are pre-determined through the user's configuration of the experiment, and will not change over the course of the market simulation.

Inventory-based models are comparatively easy to implement, as they only depend on data immediately available to the vendor. This has the advantage that rule-based agents which fall into this category are relatively simple to create and modify. Examples of \emph{Inventory-based agents} in our framework can be found in the \emph{RuleBasedCERebuyAgent}, one of the first rule-based agents we created. This agent does not take pricing decisions of its competitors in the market into account, but simply acts according to its own storage costs, always trying to keep a balance between having enough (refurbished) products to sell back to customers and keeping storage costs low. It does this by checking into which of four possible ranges the current number of stored products falls. The more products the agent already has in storage, the lower it sets the rebuy price (to `prevent' customers from returning more products), and the lower it sets the price for refurbished products, to empty the storage more quickly and prevent high storage costs. While its performance is not necessarily bad, it is still one of the weakest competitors currently available in the framework. For the full implementation logic of the agent's policy, see \Cref{fig:PolicyRuleBasedCERebuy}. For a comparison of the \emph{RuleBasedCERebuyAgent} with a more sophisticated \emph{Data-driven model} see \Cref{fig:RulebasedAgentMonitoring}.

\subsection{Data-driven models}\label{subsec:DataDrivenModels}

\emph{Data-driven} models take dynamic pricing decision-making one level further. They utilize their knowledge of the market, such as customer preferences, past sales data or competitor prices, to derive optimal pricing decisions. Aside from the aforementioned \emph{FixedPriceAgents} and the basic \emph{RuleBasedCERebuyAgent}, all of our other rule-based agents fall into this category. One of the most prominent examples of a \emph{Data-driven model} is the \emph{RuleBasedCERebuyAgentCompetitive}, an agent whose basic goal is to always try and undercut competitor prices, while also keeping track of the current amount of items in storage. In its policy, this agent first always undercuts the new price of all other competitors. Then, similar to the \emph{RuleBasedCERebuyAgent}, the agent looks at the current amount of stored products and depending on the range, sets lower prices for refurbished items and higher rebuy prices, while always trying to give customers a better deal than any of the competitors. Again, the full implementation logic of this vendor's policy is available in the Appendix, under \Cref{fig:PolicyRuleBasedCompetitive}.

Notably, all of our \emph{Data-driven models} are also \emph{Inventory-based} to a certain extent, as handling storage plays a big part in a Circular Economy market setting, where used products need to be bought back from customers and subsequently undergo refurbishment while in inventory of the company. \emph{Data-driven models} have proven to be the most competent rule-based agents in our recommerce market scenario, in particular the above described \emph{RuleBasedCERebuyAgentCompetitive}, which is able to outperform most other rule-based agents, both \emph{Inventory-based} and \emph{Data-driven models} (please refer to e.g. \Cref{fig:PPOOligopolyLineProfitsAll}).

\subsection{Game theory models}\label{subsec:GameTheory}

Game theory concerns itself with the study of models for conflict and cooperation between rational, intelligent entities~\cite{GameTheory}. It is therefore often applicable in situations where competing individuals, acting rationally and selfishly, interact with each other. In our framework, most competitors in the market are influenced in their decision-making processes by the actions of other participants of the scenario. This is especially true for the RL agents, which base their policy on the received market states, which include their competitor's actions. While none of our rule-based agents have been specifically designed to act according to game theoretic strategies, due to the fact that almost all of them consider their competitors prices in their pricing decision, and due to the nature of RL trying to maximize their own profits without regard to their competitors performance, behaviour according to Game theory can sometimes be observed. This can be observed especially well in the beginning of training sessions, where rule-based competitors often struggle to achieve high profits while the RL agent is making great losses, but are then able to increase their profits as the trained agent starts to make better decisions as well. Many examples of such behaviour can be found in the diagrams shown in \cbfref{ch:AnalysingGraphs}, such as in \Cref{fig:SACDuopolyProfitsMean}.

During training, RL agents observe the market state, which includes prices and sales data not only of themselves but also the other vendors in the market. Using this data, the algorithms try to predict how their prices will influence customer behaviour as well as the competing vendors. In some cases, depending on the concrete behaviour of the competitors, vendors may cooperate in driving prices higher together, in other cases the agents may act seemingly irrationally, lowering prices in order to force their opponents to lower theirs as well. An example of such behaviour can be seen in the development of prices for new items set by the two competing vendors in \Cref{fig:SACDuopolyMixedGraphs4}.

\subsection{Machine learning models}\label{subsec:MachineLearningModels}

All of our Reinforcement learning agents fall into this category. As they are not the focus of this thesis, we will not go into detailed explanations of the various models used. For a comparison of the performance of different RL algorithms in the context of our simulation framework, please refer to~\cite{JanThesis} The following will give a short overview over the different algorithms used in our framework.

There are two `origins' of the algorithms in our framework. In the earlier phases, \hyperref[item:QLearning]{Q-Learning} and \hyperref[item:ActorCritic]{Actor-Critic} algorithms were custom implemented by us. Later on, we used the \hyperref[item:StableBaselines]{Stable-Baselines} library to incorporate a greater number of pre-implemented algorithms into our framework. While these are not as easily configurable, they can quickly be used without much additional work.

\medskip
\noindent\textbf{Q-Learning}\phantomsection:\label{item:QLearning} \emph{Q-Learning} agents were the first RL agents we introduced in our framework, as its algorithm is one of the easier ones to implement~\cite{reinforcementLearningOverview}. The \emph{Q-Learning} algorithm used in our framework is implemented using the \emph{pytorch} framework (see \cite{Pytorch}). However, the drawback of using \emph{Q-Learning} in our framework is that it can only be applied to discrete action and state spaces. This means that when using \emph{Q-Learning}, only `whole' prices can be set by our vendors, and any decimal places must be omitted. This of course limits the framework in its realism, as the fine-tuning of prices using decimal places can be critical in influencing customer decisions. In the initial exploration-phase of our simulation framework this was not a problem, as relatively small action spaces were used, adapted to this limitation. But by now, our simulation framework also supports continuous action and state spaces, to allow more complex algorithms such as those introduced in the sections below to function. While approaches for \emph{Q-Learning} algorithms that can work with continuous action and state spaces have been presented in the past (\cite{QLearningContinuous},~\cite{QLearningContinuous2}), we have chosen not to implement such an algorithm in our framework, but rather explore approaches other than \emph{Q-Learning}, such as \emph{Actor-Critic} algorithms, introduced below.

\medskip
\noindent\textbf{Actor-Critic}\phantomsection:\label{item:ActorCritic} \emph{Actor-Critic} algorithms are more complex than Q-Learning algorithms, and have therefore been implemented later in the process. They are structured different then Q-Learning algorithms in the way that they are `split' into two parts: The \emph{actor} is responsible for selecting actions, while the \emph{critic} is responsible for critiquing  the actions taken by the \emph{actor}, thereby improving its performance~\cite{ActorCritic}. Similar to the \emph{Q-Learning} agents, the \emph{Actor-Critic} algorithms have also been implemented by us. In total, one discrete and two continuous \emph{Actor-Critic} agents can be used, in addition to those provided through \emph{Stable-Baselines}, see the next section.

\medskip
\noindent\textbf{Stable-Baselines}\phantomsection:\label{item:StableBaselines} Stable-Baselines provides a number of open-source implementations for various RL algorithms. In our framework, we use the latest version of these algorithms, through \emph{Stable-Baselines3} \cite{StableBaselines3}. The advantage when using algorithms provided through Stable-Baselines lies in the fact that they need close to no custom implementation from our end, we can instead interact with them through very simple interfaces. This cuts down on the amount of time and effort that needs to be spent developing, implementing and maintaining these powerful algorithms, and allows us to introduce a higher number of algorithms than would otherwise be possible.

\pagebreak
Currently, five different \emph{Stable-Baselines3} algorithms are used in our simulation framework, see the \emph{Stable-Baselines3} documentation~\cite{StableBaselines3Algorithms} and the referenced papers for more information about the different algorithms:
\begin{itemize}
	\item \emph{A2C}: Advantage-Actor-Critic (\cite{StableBaselines3A2C})
	\item \emph{DDPG}: Deep deterministic policy gradient (\cite{StableBaselines3DDPG})
	\item \emph{PPO}: Proximal Policy Optimization (\cite{StableBaselines3PPO})
	\item \emph{SAC}: Soft Actor-Critic (\cite{StableBaselines3SAC})
	\item \emph{TD3}: Twin-delayed deep deterministic policy gradient (\cite{StableBaselines3TD3})
\end{itemize}
We will be using some of these algorithms in our experiments and briefly compare them with our rule-based approaches using our various monitoring tools, which are introduced in the next chapter, \cbfref{ch:Approaches}.
