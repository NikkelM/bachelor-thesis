\begin{jointwork}\label{ch:SimulatingMarketplace}
	In this section we will outline the requirements and challenges of transferring the vast amount of features and parameters that influence vendor and customer decisions in real recommerce markets to our simulated market environment. We will take a brief look at different market scenarios as well as how our customers work, with a focus on the unique feauture of recommerce markets; the buyback of used items by the vendors. The focus of this chapter will however lie on the vendors, the rule based as well as the ones trained using Reinforcement Learning, comparing their features and how they fare against each other. These comparisons will be done using our various monitoring tools, which will be explained in the following chapter, \nameref{ch:Approaches}. While results of these experiments will be shortly touched upon, we will go into further detail in the closing chapter, \nameref{ch:InterpretResults}.
\end{jointwork}

\section{Market scenarios}\label{sec:MarketScenarios}

In our framework, we implemented a number of `market blueprints' both for classic Linear Economy markets, as well as for Circular Economy markets both with and without rebuy-prices enabled. There are marketplaces available for each combination of the following two features:
\begin{enumerate}
	\item Marketplace type: Linear Economy, Circular Economy, Circular Economy with rebuy-prices
	\item Market environment: Monopoly, Duopoly, Oligopoly
\end{enumerate}
The marketplace type defines the number of prices the vendor has to set. In a Linear Economy, vendors only set prices for new items, in a Circular Economy prices for refurbished items need to be set as well. In order to simulate a recommerce market, the user can choose a Circular Economy with rebuy-prices.
The market environment defines the number of competing vendors in the simulation: Monopolistic and competitive markets are available, where the Duopoly is simply a particular version of an Oligopoly. Depending on the chosen market environment, one, two, or any number of vendors can be chosen to be used in the experiment.

In the most common usecase of our framework, the training of Reinforcement-Learning agents, only the agent that is to be trained needs to be configured by the user. For this reason, each market environment is equipped with a pre-defined set of competitors that will play against the agent defined by the user. To allow for more control over the simulation, users are however also able to customize this `competitor-list' to use any vendors they want - as long as they are a valid fit for the marketplace type and the number of competitors chosen matches the market environment. In certain situations, such as the \nameref{subsec:AgentMonitoring} tool, multiple Reinforcement Learning agents can be chosen to compete on one market.\todo{Market state random shuffle}

\section{Customers}\label{sec:Customers}\todo{Alex: `Die Erklärung des Demand-Models ist sehr oberflächlich, hier würde eine genaue Angabe der Entscheidungslogik (Formeln) dem Verständnis sehr helfen'}

Customers are at the center of every type of marketplace, which makes them an integral part of our market simulation. However, since each customer in the real market is an individual with different reasonings and makes purchase decisions based upon personal preferences, modelling a realistic depiction of real-world customers proves to be very difficult and time-consuming. For this reason we decided to keep our initial implementation of the customers as simple as possible, taking into account future extension and scalability concerns.

Most customers' behaviour can be classified into one of a (non-exhaustive) number of categories, such as \emph{Perfectionistic} or \emph{Impsulsive} consumers, proposed in \cite{ShoppingStyles} (see \Cref{tab:shoppingStyles}). As we are focussing on dynamic pricing and our vendors can only change/influence the prices of their products, we decided on primarily building customers that value price over any other features a product may have, thereby incentivizing our vendors to make the most of their pricing power. This behaviour coincides with the shopping style of the \emph{Price Conscious} or\emph{`Value for money'} consumer.

To make Linear markets a little more complex and add another dimension than just the \emph{new price} of a product for vendors to consider, random quality values are assigned to each vendor's products. Customers in this economy model were modelled take this quality value into account when making their purchasing decisions, further reinforcing the shopping style mentioned above. As Circular Economy markets are inherently more complex than their linear counterparts (through the addition of two new price channels, which influence each other through their effect on customers), it was decided to remove the additional layer of quality values from these markets.

Within each simulated step, after the various vendors have set their prices, purchase decisions are made by the customers. To save on computational time, probability distributions are used to determine what part of the total number of customers will decide to take which action. See below the way these distributions are calculated for an exemplary Circular Economy marketplace:

Let \(P_{in}\) be the price of the new and \(P_{ir}\) the price of the refurbished product of vendor \(i\). Using these prices, we define \(R_{in}\) as the \emph{preference ratio} regarding \(P_{in}\):
\begin{equation}
	R_{in} := \frac{10}{P_{in}} - e^{P_{in} - 8}
\end{equation}
and similarly, \(R_{ir}\) as the \emph{preference ratio} regarding \(P_{ir}\):
\begin{equation}
	R_{ir} := \frac{5.5}{P_{ir}} - e^{P_{ir} - 5}
\end{equation}

In order to be usable as a probability distribution, the different ratios need to add up to \(1\). We use \emph{softmax} to normalize our values, after which each \emph{preference ratio} will be within the interval \((0,1)\), with all ratios adding up to \(1\) as needed.

Following this, the market uses a \emph{multinomial} distribution to draw \(n\) samples, with \(n\) being the number of customers making a pruchasing decision in this step of the simulation.

As mentioned above, the current decisionmaking process of customers in our simulation is still quite basic. \nameref{sec:DivergingFromRealMarket} introduces a number of parameters and circumstances that can be used to make customer behaviour more realistic. Through the modular approach when building the framework, any customer behaviour implemented in the future can easily be added to the pool of available options, as long as it manifests in the form of a probability distribution, at which point the number of customers can be split between different distributions when drawing from the \emph{multinomial} distribution (or any other distribution if so desired).

\subsection*{Owners}

Once a customer has decided to buy a product from any of the available vendors, they turn into an \emph{owner}. In the next step of the simulation, they are offered the option of selling their now used product back to one of the vendors\todo{Formula}. If they decide to do so, the vendor pays them the advertised \emph{re-buy price} and adds the used product to their inventory, from where it can then be sold as a refurbished product in the next step.

\section{Vendors}\label{sec:ExplainVendors}\todo{Alex: `Die Erklärung der Methoden die ihr zum Pricing benutzt leidet ein wenig an der selben Angewohnheit, da werden Klassennamen genannt, mit denen der Leser potenziell nichts anfangen kann, aber eine Erklärung was die einzelnen Methoden tun fehlt fast völlig, oder benutzt sehr unspezifiscihe Formulierungen wie `This agent does not take pricing decisions of its competitors in the market into account, but simply acts according to its own storage costs.' Der letzte Satz gibt zwar eine gute Einleitung ab, aber erklärt nicht was der Algorithmus eigentlich tut. Wenn die Information welche Algorithmen zur Anwendung kamen relevant ist, sollte die Erklärung hier nicht nur eine interne Bezeichnung nennen, sondern wenigstens eine grobe Erklärung der Funktionsweise enthalten (wenn auch zB im Falle der RL Algos natürlich nicht so spezifisch wie in Jans Arbeit).'}\todo{Alex: `Welche dieser Methoden kommen auf der `ausgewerteten' Seite zum Einsatz? Welche sind nur Gegenspieler für die Auswertung? Das würde ich noch sauber abgrenzen.'}

Vendors are the main focus of our market simulation.

While our framework will not be able to reproduce all types of pricing models used in the real market, we strive to model as many different models as possible (and feasable in the scope of the project). We will use the following four types of dynamic pricing models, as described in `Dynamic pricing models for electronic business'~\cite{dynamicPricingModels}, to group our different approaches and see how each type performs in the context of our simulation framework:

\begin{enumerate}
	\item \emph{Inventory-based models}: These are pricing models which are based on inventory levels and similar parameters, such as the number of items in circulation. In our framework, almost all rule-based agents consider their inventory levels when deciding which prices to set. The only exception to this rule are the simplest of our agents, the \emph{FixedPriceAgents}, which will always set the same prices, no matter the current market state and competitor actions. We chose to implement this rather incompetent type of agent, as there are still many vendors in the real market which do not make use of dynamic pricing models, but use static pricing methods like these \emph{FixedPriceAgents} instead. Not including these in our framework would therefore remove a fraction of realism.

	      Inventory-based models are comparatively easy to implement, as they only depend on data immediately available to the vendor. This has the advantage that rule based agents based on this technique are relatively simple to create and modify. Examples of \emph{Inventory-based agents} in our framework can be found in the \emph{RuleBasedCEAgent}, one of \emph{RuleBasedCEAgent} the first rule based agents we created. This agent does not take pricing decisions of its competitors in the market into account, but simply acts according to its own storage costs. While its performance is not necessarily bad, it is still one of the weakest competitors currently available in the framework. \todo{Compare this agent with other agents, RL, FixedPrice, better rule-based!}

	\item \emph{Data-driven models}: \emph{Data-driven} models take dynamic pricing decisionmaking one level further. They utilize their knowledge of the market, such as customer preferences, past sales data or competitor prices, to derive optimal pricing decisions. Aside from the aforementioned \emph{FixedPriceAgents} and the basic \emph{RuleBasedCEAgent}, all of our other rule based agents fall into this category. One of the most prominent examples of a \emph{Data-driven model} is the \emph{RuleBasedCERebuyAgentStorageMinimizer}. This agent bases its prices on two major factors. First, it reacts to its competitors prices by matching its initial prices to the median price of its competitors, and then adjusts them according to current inventory. Notably, all of our \emph{Data-driven models} are also \emph{Inventory-based} to a certain extent, as handling storage plays a big part in a Circular Economy market setting, where used products need to be bought back from customers and subsequently undergo refurbishment while in inventory of the company. \emph{Data-driven models} have proven to be the most competent rule based agents in our recommerce market scenario, in particular the above described \emph{RuleBasedCERebuyAgentStorageMinimizer}, which only consists of ten lines of code but is still able to outperform seemingly more complex \emph{Inventory-based models} \todo{Graphs}.

	\item \emph{Game theory models}:\label{bullet:GameTheory}Game theory concerns itself with the study of models for conflict and cooperation between rational, intelligent entities~\cite{GameTheory}. It is therefore often applicable in situations where competing individuals, acting rationally and selfishly interact with each other. In our framework, competitors in the market are influenced in their decisionmaking processes by the actions of other participants of the scenario. While none of our rule based agents have been specifically designed to act according to game theoretic strategies, due to the fact that almost all of them consider their competitors prices in their pricing decision, and due to the nature of Reinforcement Learning trying to maximize their own profits without regard to their competitors performance, behaviour according to Game theory can sometimes be observed. \todo{Need graphs to back this claim up! Especially price-graphs, moving up and down etc.} During training, Reinforcement Learning agents observe the market state, which includes prices and sales data not only of themselves but also the other vendors in the market. Using this data, the algorithms try to predict how their prices will influence customer behaviour as well as the competing vendors. In some cases, depending on the concrete behaviour of the competitors, vendors may cooperate in driving prices higher together, in other cases the agents may act more selfishly, sacrificing own profits to stay ahead of their competitors in other regards. \todo{Graphs}

	\item \emph{Machine learning models}:\todo{Alex: `Ich würde die SB Algorithmen hier lieber bei den anderen mit einordnen, da gehören sie ja schematisch dazu. Dann am Ende kurz erwähnen, dass die Implementierung dieser Algos aus der SB Lib kam. Die Kategorie Q-Learning wäre mit Value-Learning oder Temporal Difference Algorithms besser beschriftet, das wären die beiden üblichen Oberbegriffe für die Algorithmen die auf Basis vergangener Beobachtungen (Q-) Values schätzen'}\todo{Make sure to get some comparisons between these algorithms going!} All of our Reinforcement learning agents fall into the category \emph{Machine learning models}. As it is not the focus of this thesis, we will not go into detailed explanations of the various models used, but instead focus on how they perform in relation to each other. The algorithms used by us can be grouped together into the following three categories:
	      \begin{itemize}
		      \item \emph{Q-Learning}: \emph{Q-Learning} agents were the first Reinforcement Learning agents we introduced in our framework, as it is one of the easier Reinforcement-Learning algorithms to implement~\cite{reinforcementLearningOverview}. However, the drawback of using \emph{Q-Learning} in our framework is that it can only be applied to discrete action and state spaces.\todo{This must not be `cited', right?} In the initial exploration-phase of our simulation framework this was not a problem, as relatively small action spaces were used, adapted to this limitation. This means that when using \emph{Q-Learning}, only `whole' prices can be set, and any decimal places must be omitted. This of course limits the framework in its realism, as the fine-tuning of prices using decimal places can be critical in influencing customer decisions. It should be mentioned that while this limitation is known to us, our simulation framework still operates on discrete action and state spaces, both to allow algorithms like \emph{Q-Learning} to still function and to not overly complicate the simulation, as many of the other components (such as customer behaviour, see \nameref{sec:Customers}) are still very basic in their nature as well. The \emph{Q-Learning} used in our framework have been custom implemented by us, using the \emph{pytorch}\footnote[0][-0.2]{https://pytorch.org/docs/stable/index.html} framework. While approaches for \emph{Q-Learning} algorithms that can work with continuous action and state spaces have been presented in the past (\cite{QLearningContinuous},~\cite{QLearningContinuous2}), we have chosen not to implement such an algorithm in our framework, but rather explore more approaches other than \emph{Q-Learning}, introduced below.
		      \item \emph{Actor-Critic}: \emph{Actor-Critic} algorithms are more complex than Q-Learning algorithms, and have therefore been implemented later in the process. They are structured different then Q-Learning algorithms in the way that they are `split' into two parts: The \emph{actor} is responsible for selecting actions, while the \emph{critic} is responsible for critizing the actions taken by the \emph{actor}, thereby improving its performance~\cite{ActorCritic}. Similar to the \emph{Q-Learning} agents, the \emph{Actor-Critic} algorithms have also been implemented by us. In total, one discrete and two continuous \emph{Actor-Critic} agents can be used, in addition to those provided through \emph{Stable-Baselines}, see the next section.
		      \item \emph{Stable-Baselines}: Stable-Baselines provides a number of open-source implementations for various Reinforcement-Learning algorithms. In our framework, we use the latest version of these algorithms, through \emph{Stable-Baselines3}~\cite{StableBaselines3}. The advantage when using algorithms provided through Stable-Baselines lies in the fact that they need close to no custom implementation from our end, we can instead interact with them through very simple interfaces. This cuts down on the amount of time and effort that needs to be spent developing, implementing and maintaining these powerful algorithms, and allows us to introduce a higher number of algorithms than would otherwise be possible. Currently, five different \emph{Stable-Baselines3} algorithms are used in our simulation framework, see the \emph{Stable-Baselines3} documentation for more information~\cite{StableBaselines3Algorithms}:
		            \begin{itemize}
			            \item \emph{A2C}: Advantage-Actor-Critic
			            \item \emph{DDPG}: Deep deterministic policy gradient
			            \item \emph{PPO}: Proximal Policy Optimization
			            \item \emph{SAC}: Soft Actor-Critic
			            \item \emph{TD3}: Twin-delayed deep deterministic policy gradient
		            \end{itemize}
		            We will be using some of these algorithms in our experiments and briefly compare them with our Rule-Based approaches using our various monitoring tools, which are introduced in \nameref{ch:Approaches}.
	      \end{itemize}
\end{enumerate}


\section{Diverging from the real market}\label{sec:DivergingFromRealMarket}\todo{This focusses mostly on customer behaviour, should it perhaps be a subsection of the customer section?}

We do not claim in any way that our simulation framework is exhaustive or complete. There are a number of parameters influencing market dynamics, small and great, which have not been modelled due to time constraints or their unpredictability in their effect on the market state or customer decisions. Seasonality of demand, customer retention, loyalty through branding and unforeseen effects on market dynamics such as global events (for example, the Covid-19 pandemic) are just some of the places where our market simulation diverges from the real market, following its inability to model these circumstances. However, all of these factors are either very rare in nature (global, unforeseen events), only applicable to a subset of markets (brand loyalty) or can initially be discarded as their effect on market dynamics is predictable, making them a lower priority than other parameters (seasonality).

The factors mentioned above all have an impact on both Linear and Circular markets, but there are also a number of parameters that are specific to Circular or even recommerce markets in particular. In the modern recommerce market, more and more consumers make their initial purchasing decisions with the product's eventual resale value already in mind \cite{ShoppingResaleValue}\todo{Table from paper in appendix}. Additionally, a great number of different motivations for choosing second-hand or refurbished products over traditional new ones can be identified. An exemplary study conducted in 2008 (\cite{SecondHandMotives}) classifies\todo{classifies or classified?} such motivations into 15 different categories, all of which could be used to add dimensions to customer behaviour in our simulation framework, thereby making the whole simulation more realistic.

Of course, this results in the fact that our framework can not be used out-of-the-box for any kind of market. However, great care was taken during the development process to build each part of the simulation in a modular way, so that new functionality can easily be integrated into it.\todo{Diagram for modularity here/reference here} Marketplaces, customers and vendors (rule-based as well as those using Reinforcement learning) have all been implemented as classes disconnected from each other, so that new variants of each can easily be added to the pool of available options to be used in experiments. The same goes for our monitoring tools, all of which have been built to work with any (valid) combination of input parameters.

The lack of realism on certain areas of the framework does not directly impact the way that our monitoring tools work or can be used. It must however be noted that the interpretation of results must always be based on the knowledge of these limitations.