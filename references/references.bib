@article{circularEconomyDefinition,
title = {Conceptualizing the circular economy: An analysis of 114 definitions},
journal = {Resources, Conservation and Recycling},
volume = {127},
pages = {221-232},
year = {2017},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921344917302835},
author = {Julian Kirchherr and Denise Reike and Marko Hekkert},
keywords = {Circular economy, 4R framework, Sustainable development, Definitions, Content analysis},
abstract = {The circular economy concept has gained momentum both among scholars and practitioners. However, critics claim that it means many different things to different people. This paper provides further evidence for these critics. The aim of this paper is to create transparency regarding the current understandings of the circular economy concept. For this purpose, we have gathered 114 circular economy definitions which were coded on 17 dimensions. Our findings indicate that the circular economy is most frequently depicted as a combination of reduce, reuse and recycle activities, whereas it is oftentimes not highlighted that CE necessitates a systemic shift. We further find that the definitions show few explicit linkages of the circular economy concept to sustainable development. The main aim of the circular economy is considered to be economic prosperity, followed by environmental quality; its impact on social equity and future generations is barely mentioned. Furthermore, neither business models nor consumers are frequently outlined as enablers of the circular economy. We critically discuss the various circular economy conceptualizations throughout this paper. Overall, we hope to contribute via this study towards the coherence of the circular economy concept; we presume that significantly varying circular economy definitions may eventually result in the collapse of the concept.}
}

@article{reinforcementLearningOverview,
  author          = {Leslie Pack Kaelbling and Michael L. Littman and Andrew W. Moore},
  journal         = JAIR,
  title           = {Reinforcement Learning: A Survey},
  volume          = {4},
  doi             = {https://doi.org/10.1613/jair.301},
  url			  = {https://www.jair.org/index.php/jair/article/view/10166},
  pages     	  = {237--285},
  year            = {1996}
}

@article{dynamicPricingModels,
author={Narahari, Y. and Raju, C. V. L. and Ravikumar, K. and Shah, Sourabh},
title={Dynamic pricing models for electronic business},
journal={Sadhana},
year={2005},
month={Apr},
day={01},
volume={30},
number={2},
pages={231-256},
abstract={Dynamic pricing is the dynamic adjustment of prices to consumers depending upon the value these customers attribute to a product or service. Today's digital economy is ready for dynamic pricing; however recent research has shown that the prices will have to be adjusted in fairly sophisticated ways, based on sound mathematical models, to derive the benefits of dynamic pricing. This article attempts to survey different models that have been used in dynamic pricing. We first motivate dynamic pricing and present underlying concepts, with several examples, and explain conditions under which dynamic pricing is likely to succeed. We then bring out the role of models in computing dynamic prices. The models surveyed include inventory-based models, data-driven models, auctions, and machine learning. We present a detailed example of an e-business market to show the use of reinforcement learning in dynamic pricing.},
issn={0973-7677},
doi={10.1007/BF02706246},
url={https://doi.org/10.1007/BF02706246}
}

@book{GameTheory,
  title={Game theory: analysis of conflict},
  author={Myerson, Roger B},
  year={1997},
  publisher={Harvard university press},
  pages={1}
}

@InProceedings{QLearningContinuous,
author="Gaskett, Chris
and Wettergreen, David
and Zelinsky, Alexander",
editor="Foo, Norman",
title="Q-Learning in Continuous State and Action Spaces",
booktitle="Advanced Topics in Artificial Intelligence",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="417--428",
abstract="Q-learning can be used to learn a control policy that maximises a scalar reward through interaction with the environment. Q-learning is commonly applied to problems with discrete states and actions. We describe a method suitable for control tasks which require continuous actions, in response to continuous states. The system consists of a neural network coupled with a novel interpolator. Simulation results are presented for a non-holonomic control task. Advantage Learning, a variation of Q-learning, is shown enhance learning speed and reliability for this task.",
isbn="978-3-540-46695-6"
}

@Article{QLearningContinuous2,
author={Mill{\'a}n, Jos{\'e} del R.
and Posenato, Daniele
and Dedieu, Eric},
title={Continuous-Action Q-Learning},
journal={Machine Learning},
year={2002},
month={Nov},
day={01},
volume={49},
number={2},
pages={247-265},
abstract={This paper presents a Q-learning method that works in continuous domains. Other characteristics of our approach are the use of an incremental topology preserving map (ITPM) to partition the input space, and the incorporation of bias to initialize the learning process. A unit of the ITPM represents a limited region of the input space and maps it onto the Q-values of M possible discrete actions. The resulting continuous action is an average of the discrete actions of the ``winning unit'' weighted by their Q-values. Then, TD($\lambda$) updates the Q-values of the discrete actions according to their contribution. Units are created incrementally and their associated Q-values are initialized by means of domain knowledge. Experimental results in robotics domains show the superiority of the proposed continuous-action Q-learning over the standard discrete-action version in terms of both asymptotic performance and speed of learning. The paper also reports a comparison of discounted-reward against average-reward Q-learning in an infinite horizon robotics task.},
issn={1573-0565},
doi={10.1023/A:1017988514716},
url={https://doi.org/10.1023/A:1017988514716}
}

